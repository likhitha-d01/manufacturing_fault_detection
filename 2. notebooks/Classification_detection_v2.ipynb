{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification_detection_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "eOKnaghloA6g",
        "AJ5iCdZVKsaN",
        "sFXM1B9Hjqri",
        "RGdcDgaSkMEi",
        "nGZyKHQDL9V5",
        "Hk7FthpMV4Kf"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko_VStSBjWVP"
      },
      "source": [
        "# Classification and detection model - Version 2\n",
        "\n",
        "The classification backbone of the model is Resnet network along with the detection activation layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOKnaghloA6g"
      },
      "source": [
        "## Building the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCidrrOuy3qZ"
      },
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        " \n",
        "tf.config.run_functions_eagerly(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsY0ql4hy9iN"
      },
      "source": [
        "def identity_block(X, f, filters, stage, block):\n",
        "    \"\"\"\n",
        "    Implementation of the identity block as defined in Figure 3\n",
        "\n",
        "    Arguments:\n",
        "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
        "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
        "    stage -- integer, used to name the layers, depending on their position in the network\n",
        "    block -- string/character, used to name the layers, depending on their position in the network\n",
        "\n",
        "    Returns:\n",
        "    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n",
        "    \"\"\"\n",
        "    policy = tf.keras.mixed_precision.experimental.Policy(\n",
        "        'mixed_float16') \n",
        "    \n",
        "    # defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "\n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "\n",
        "    # Save the input value. You'll need this later to add back to the main path.\n",
        "    X_shortcut = X\n",
        "\n",
        "    # First component of main path\n",
        "    X = keras.layers.Conv2D(filters=F1, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2a',\n",
        "                            kernel_initializer=keras.initializers.glorot_uniform(seed=0), dtype=policy)(X)\n",
        "    X = keras.layers.BatchNormalization(axis=3, name=bn_name_base + '2a', dtype=policy)(X)\n",
        "    X = keras.layers.Activation('relu')(X)\n",
        "\n",
        "    # Second component of main path (≈3 lines)\n",
        "    X = keras.layers.Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same', name=conv_name_base + '2b',\n",
        "                            kernel_initializer=keras.initializers.glorot_uniform(seed=0), dtype=policy)(X)\n",
        "    X = keras.layers.BatchNormalization(axis=3, name=bn_name_base + '2b', dtype=policy)(X)\n",
        "    X = keras.layers.Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path (≈2 lines)\n",
        "    X = keras.layers.Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2c',\n",
        "                            kernel_initializer=keras.initializers.glorot_uniform(seed=0), dtype=policy)(X)\n",
        "    X = keras.layers.BatchNormalization(axis=3, name=bn_name_base + '2c', dtype=policy)(X)\n",
        "\n",
        "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
        "    X = keras.layers.Add()([X, X_shortcut])\n",
        "    X = keras.layers.Activation('relu')(X)\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "def convolutional_block(X, f, filters, stage, block, s=2):\n",
        "    \"\"\"\n",
        "    Implementation of the convolutional block as defined in Figure 4\n",
        "\n",
        "    Arguments:\n",
        "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
        "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
        "    stage -- integer, used to name the layers, depending on their position in the network\n",
        "    block -- string/character, used to name the layers, depending on their position in the network\n",
        "    s -- Integer, specifying the stride to be used\n",
        "\n",
        "    Returns:\n",
        "    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n",
        "    \"\"\"\n",
        "\n",
        "    policy = tf.keras.mixed_precision.experimental.Policy(\n",
        "        'mixed_float16') \n",
        "\n",
        "    # defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "\n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "\n",
        "    # Save the input value\n",
        "    X_shortcut = X\n",
        "\n",
        "    ##### MAIN PATH #####\n",
        "    # First component of main path\n",
        "    X = keras.layers.Conv2D(F1, (1, 1), strides=(s, s), name=conv_name_base + '2a', \n",
        "                            kernel_initializer=keras.initializers.glorot_uniform(seed=0), dtype=policy)(X)\n",
        "    X = keras.layers.BatchNormalization(axis=3, name=bn_name_base + '2a', dtype=policy)(X)\n",
        "    X = keras.layers.Activation('relu')(X)\n",
        "\n",
        "    # Second component of main path (≈3 lines)\n",
        "    X = keras.layers.Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same', name=conv_name_base + '2b',\n",
        "                            kernel_initializer=keras.initializers.glorot_uniform(seed=0), dtype=policy)(X)\n",
        "    X = keras.layers.BatchNormalization(axis=3, name=bn_name_base + '2b', dtype=policy)(X)\n",
        "    X = keras.layers.Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path (≈2 lines)\n",
        "    X = keras.layers.Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2c',\n",
        "                            kernel_initializer=keras.initializers.glorot_uniform(seed=0), dtype=policy)(X)\n",
        "    X = keras.layers.BatchNormalization(axis=3, name=bn_name_base + '2c', dtype=policy)(X)\n",
        "\n",
        "    ##### SHORTCUT PATH #### (≈2 lines)\n",
        "    X_shortcut = keras.layers.Conv2D(filters=F3, kernel_size=(1, 1), strides=(s, s), padding='valid', name=conv_name_base + '1',\n",
        "                                     kernel_initializer=keras.initializers.glorot_uniform(seed=0), dtype=policy)(X_shortcut)\n",
        "    X_shortcut = keras.layers.BatchNormalization(axis=3, name=bn_name_base + '1', dtype=policy)(X_shortcut)\n",
        "\n",
        "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
        "    X = keras.layers.Add()([X, X_shortcut])\n",
        "    X = keras.layers.Activation('relu')(X)\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "def build_model(input_shape, out_type, number_of_attributes):\n",
        "\n",
        "    policy = tf.keras.mixed_precision.experimental.Policy(\n",
        "        'mixed_float16')  # sets values to be float16 for nvidia 2000,3000 series GPUs, plus others im sure\n",
        "\n",
        "    # Define the input as a tensor with shape input_shape\n",
        "    X_input = keras.layers.Input(input_shape)\n",
        "\n",
        "    # Zero-Padding\n",
        "    X = keras.layers.ZeroPadding2D((3, 3), dtype=policy)(X_input)\n",
        "\n",
        "    # Stage 1\n",
        "    X = keras.layers.Conv2D(64, (7, 7), strides=(2, 2), name='conv1',\n",
        "                            kernel_initializer=keras.initializers.glorot_uniform(seed=0), dtype=policy)(X)\n",
        "    X = keras.layers.BatchNormalization(axis=3, name='bn_conv1', dtype=policy)(X)\n",
        "    X = keras.layers.Activation('relu')(X)\n",
        "    X = keras.layers.MaxPooling2D((3, 3), strides=(2, 2), dtype=policy)(X)\n",
        "\n",
        "    # Stage 2\n",
        "    X = convolutional_block(X, f=3, filters=[16, 16, 32], stage=2, block='a', s=1)\n",
        "    X = identity_block(X, 3, [16, 16, 32], stage=2, block='b')\n",
        "    X = identity_block(X, 3, [16, 16, 32], stage=2, block='c')\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # Stage 3 (≈4 lines)\n",
        "    X = convolutional_block(X, f=3, filters=[32, 32, 64], stage=3, block='a', s=2)\n",
        "    X = identity_block(X, 3, [32, 32, 64], stage=3, block='b')\n",
        "    X = identity_block(X, 3, [32, 32, 64], stage=3, block='c')\n",
        "    X = identity_block(X, 3, [32, 32, 64], stage=3, block='d')\n",
        "\n",
        "    # Stage 4 (≈6 lines)\n",
        "    X = convolutional_block(X, f=3, filters=[64, 64, 128], stage=4, block='a', s=2)\n",
        "    X = identity_block(X, 3, [64, 64, 128], stage=4, block='b')\n",
        "    X = identity_block(X, 3, [64, 64, 128], stage=4, block='c')\n",
        "    X = identity_block(X, 3, [64, 64, 128], stage=4, block='d')\n",
        "    X = identity_block(X, 3, [64, 64, 128], stage=4, block='e')\n",
        "    X = identity_block(X, 3, [64, 64, 128], stage=4, block='f')\n",
        "\n",
        "    # Stage 5 (≈3 lines)\n",
        "    X = convolutional_block(X, f=3, filters=[128, 128, 256], stage=5, block='a', s=2)\n",
        "    X = identity_block(X, 3, [128, 128, 256], stage=5, block='b')\n",
        "    X = identity_block(X, 3, [128, 128, 256], stage=5, block='c')\n",
        "\n",
        "    # AVGPOOL (≈1 line). Use \"X = AveragePooling2D(...)(X)\"\n",
        "    #X = keras.layers.AveragePooling2D((2, 2), name=\"avg_pool\")(X)\n",
        "\n",
        "    X = keras.layers.Conv2D(16, 1, activation=\"selu\", kernel_initializer=\"lecun_normal\", dtype=policy)(X)\n",
        "    #new\n",
        "    X = keras.layers.Conv2DTranspose(1, 3, activation='linear', dtype=policy)(X)\n",
        "    #new\n",
        "    X = keras.layers.Conv2DTranspose(1, 3, activation='linear', dtype=policy)(X)\n",
        "    #X = keras.layers.Conv2D(1, 1, activation='linear')(X)\n",
        "    X_map = keras.activations.sigmoid(X)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # output layer\n",
        "\n",
        "    # new - changed X to X_map\n",
        "    X = keras.layers.Flatten()(X_map)\n",
        "    y = keras.layers.Dense(number_of_attributes, activation='softmax', name='fc' + str(number_of_attributes),\n",
        "                           kernel_initializer=keras.initializers.glorot_uniform(seed=0), dtype=policy)(X)\n",
        "\n",
        "    optimizer = keras.optimizers.SGD(lr = 0.0001, momentum=0.01, nesterov=True)\n",
        "    metrics = [\n",
        "        keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
        "        #keras.metrics.Precision(name='precision'),\n",
        "        #keras.metrics.Recall(name='recall'),\n",
        "        #keras.metrics.AUC(name='auc'),\n",
        "    ]\n",
        "\n",
        "    if out_type == \"classify\":\n",
        "        model = keras.models.Model(inputs=X_input, outputs=y)\n",
        "        model.compile(optimizer=optimizer,\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=metrics)\n",
        "        \n",
        "    if out_type == \"map\":\n",
        "        model = keras.models.Model(inputs=X_input, outputs=X_map)\n",
        "        model.compile(optimizer=optimizer,\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=metrics)\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ5iCdZVKsaN"
      },
      "source": [
        "## CASTING DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDYvlKbxKxWq"
      },
      "source": [
        "### Pre-processing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hcSgjz1bx-x"
      },
      "source": [
        "dataset_dir = '/content/drive/MyDrive/Major Project/Datasets/datasets'\n",
        "\n",
        "filename = 'steel_defect_cls_products.tar.xz'\n",
        "tar_file = os.path.join(dataset_dir, filename)\n",
        "\n",
        "my_tar = tarfile.open(tar_file)\n",
        "my_tar.extractall(dataset_dir) # specify which folder to extract to\n",
        "my_tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZIlPBW4KwpV"
      },
      "source": [
        "import os\n",
        "import tarfile\n",
        "import shutil\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import random\n",
        "\n",
        "dataset_dir = '/content/drive/MyDrive/Major Project/Datasets/datasets'\n",
        "\n",
        "model_dir = '/content/drive/MyDrive/Major Project/Models/V2'\n",
        "\n",
        "if not os.path.exists(model_dir):\n",
        "    os.mkdir(model_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZZmnFglcYN0"
      },
      "source": [
        "dataset = os.path.join(dataset_dir, 'casting_dataset')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocSXFo-acZGo"
      },
      "source": [
        "generator = keras.preprocessing.image.ImageDataGenerator(\n",
        "                            rotation_range=10,\n",
        "                            width_shift_range=0.2,\n",
        "                            height_shift_range=0.2,\n",
        "                            brightness_range = [0.5,1.0],\n",
        "                            zoom_range=0.1,\n",
        "                            rescale=1./255,\n",
        "                            fill_mode=\"nearest\",\n",
        "                            cval=1.0,\n",
        "                            horizontal_flip=True,\n",
        "                            vertical_flip=True,\n",
        "                            validation_split=0.1,\n",
        "                            dtype='float64',\n",
        "                        )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7yF7UehcZJ4"
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_generator = generator.flow_from_directory( os.path.join(dataset, 'train'), \n",
        "                                                target_size=(224, 224), \n",
        "                                                batch_size=batch_size,\n",
        "                                                class_mode='categorical',\n",
        "                                                subset='training',\n",
        "                                                shuffle=True) \n",
        "                                            \n",
        "val_generator = generator.flow_from_directory( os.path.join(dataset, 'train'), \n",
        "                                                target_size=(224, 224), \n",
        "                                                batch_size=batch_size,\n",
        "                                                class_mode='categorical',\n",
        "                                                subset='validation',\n",
        "                                                shuffle=True) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9mCWia8c7X5"
      },
      "source": [
        "attribute_to_idx = train_generator.class_indices\n",
        "idx_to_attribute = {value:key for key,value in attribute_to_idx.items()}\n",
        "\n",
        "print(attribute_to_idx)\n",
        "print(idx_to_attribute)\n",
        "\n",
        "number_of_attributes = len(idx_to_attribute)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xO_fDP0JfNCS"
      },
      "source": [
        "count_dict = {}\n",
        "\n",
        "for category in os.listdir(os.path.join(dataset, 'train')):\n",
        "\n",
        "    count_dict[category] = len(os.listdir(os.path.join(dataset, 'train', category)))\n",
        "\n",
        "print(count_dict)\n",
        "\n",
        "cutoff = 0\n",
        "\n",
        "if len(count_dict) == 2:\n",
        "\n",
        "    cutoff = min(count_dict.values())\n",
        "\n",
        "if len(count_dict) > 2:\n",
        "\n",
        "    cutoff = np.median(list(count_dict.values()))\n",
        "\n",
        "print(cutoff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vejsA-Bagtvz"
      },
      "source": [
        "#### Balancing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW1AIkW3gwQm"
      },
      "source": [
        "new_dataset = os.path.join(dataset_dir, 'casting_dataset_balanced')\n",
        "\n",
        "if not os.path.exists(new_dataset):\n",
        "    os.mkdir(new_dataset)\n",
        "\n",
        "for split in os.listdir(dataset):\n",
        "\n",
        "    print(\" \", split)\n",
        "    split_path = os.path.join(dataset, split)\n",
        "    new_split_path = os.path.join(new_dataset, split)\n",
        "\n",
        "    if not os.path.exists(new_split_path):\n",
        "        os.mkdir(new_split_path)\n",
        "\n",
        "    for category in os.listdir(split_path):\n",
        "\n",
        "        print(\"     \", category)\n",
        "        \n",
        "        category_path = os.path.join(split_path, category)\n",
        "        new_category_path = os.path.join(new_split_path, category)\n",
        "\n",
        "        if not os.path.exists(new_category_path):\n",
        "            os.mkdir(new_category_path)\n",
        "        \n",
        "        images = os.listdir(category_path)\n",
        "\n",
        "        for i in range(2):\n",
        "            random.shuffle(images)\n",
        "\n",
        "        if len(images) > cutoff:\n",
        "\n",
        "            images = images[:cutoff]\n",
        "        \n",
        "        for image in tqdm(images):\n",
        "\n",
        "            source = os.path.join(category_path, image)\n",
        "            destination = os.path.join(new_category_path, image)\n",
        "\n",
        "            shutil.copyfile(source, destination)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HrjIoqOr9Sp"
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "new_dataset = os.path.join(dataset_dir, 'casting_dataset_balanced')\n",
        "\n",
        "train_generator = generator.flow_from_directory( os.path.join(new_dataset, 'train'), \n",
        "                                                target_size=(224, 224), \n",
        "                                                batch_size=batch_size,\n",
        "                                                class_mode='categorical',\n",
        "                                                subset='training',\n",
        "                                                shuffle=True) \n",
        "                                            \n",
        "val_generator = generator.flow_from_directory( os.path.join(new_dataset, 'train'), \n",
        "                                                target_size=(224, 224), \n",
        "                                                batch_size=batch_size,\n",
        "                                                class_mode='categorical',\n",
        "                                                subset='validation',\n",
        "                                                shuffle=True) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bstjmi9ptPNT"
      },
      "source": [
        "attribute_to_idx = train_generator.class_indices\n",
        "idx_to_attribute = {value:key for key,value in attribute_to_idx.items()}\n",
        "\n",
        "print(attribute_to_idx)\n",
        "print(idx_to_attribute)\n",
        "\n",
        "number_of_attributes = len(idx_to_attribute)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqneUq65LCM0"
      },
      "source": [
        "### Training model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okBDv1VnLDmm"
      },
      "source": [
        "map_model = build_model(\"map\", number_of_attributes)\n",
        "model = build_model(\"classify\", number_of_attributes)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFEhzgDDbCzh"
      },
      "source": [
        "epochs = 50\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', patience=5, min_lr = 1e-7, factor=0.5, verbose=1)\n",
        "\n",
        "history = model.fit(train_generator, \n",
        "                    epochs= epochs,\n",
        "                    validation_data = val_generator,\n",
        "                    verbose=1,\n",
        "                    callbacks=[reduce_lr]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ics6fTYmdLBB"
      },
      "source": [
        "model.save(os.path.join(model_dir, 'casting_V1.h5'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8yF6vwfdMhj"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['lr'])\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['precision'])\n",
        "plt.plot(history.history['val_precision'])\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['recall'])\n",
        "plt.plot(history.history['val_recall'])\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['auc'])\n",
        "plt.plot(history.history['val_auc'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1P0NR7LLT6y"
      },
      "source": [
        "### Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DELw_VzGLWW3"
      },
      "source": [
        "model = keras.models.load_model(os.path.join(model_dir, 'casting_V2.h5'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFZtTz4RiDAO"
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "test_generator = generator.flow_from_directory( os.path.join(new_dataset, 'test'), \n",
        "                                                target_size=(224, 224), \n",
        "                                                batch_size=batch_size,\n",
        "                                                class_mode='categorical') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWmQloFriDFA"
      },
      "source": [
        "scores = model.evaluate_generator(test_generator, verbose=1)\n",
        "scores_keys = ['loss', 'accuracy', 'precision', 'recall', 'auc']\n",
        "\n",
        "for key,score in zip(scores_keys, scores):\n",
        "\n",
        "    print(key, ':', score)\n",
        "\n",
        "\"\"\"\n",
        "50 eps\n",
        "\n",
        "loss : 0.01628803461790085\n",
        "accuracy : 0.9959127902984619\n",
        "precision : 0.9959127902984619\n",
        "recall : 0.9959127902984619\n",
        "auc : 0.9985383749008179\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFXM1B9Hjqri"
      },
      "source": [
        "#### Visualise the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOUAgnQMiDHx"
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "for idx, (images, output) in enumerate(test_generator):\n",
        "\n",
        "    if idx == 2:\n",
        "        break\n",
        "\n",
        "    for i in range(batch_size):\n",
        "\n",
        "        image = images[i]\n",
        "\n",
        "        cv2_imshow(image * 255)\n",
        "\n",
        "        preds = model.predict(np.expand_dims(image, axis=0))\n",
        "\n",
        "        actual_value = np.argmax(output[i], axis=0)\n",
        "\n",
        "        predicted_value = np.argmax(preds, axis=1)[0]\n",
        "\n",
        "        confidence = preds[0][predicted_value] * 100\n",
        "\n",
        "        print(\"Confidence\", confidence, \"%\")\n",
        "        print(\"Actual_value\", idx_to_attribute[int(actual_value)])\n",
        "        print(\"Predicted value\", idx_to_attribute[int(predicted_value)])\n",
        "    \n",
        "    idx += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGdcDgaSkMEi"
      },
      "source": [
        "#### Visualise activation maps\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skqtzpp3kO_V"
      },
      "source": [
        "def get_img_array(img_path, size):\n",
        "    # `img` is a PIL image of size 299x299\n",
        "    img = keras.preprocessing.image.load_img(img_path, target_size=size)\n",
        "    # `array` is a float32 Numpy array of shape (299, 299, 3)\n",
        "    array = keras.preprocessing.image.img_to_array(img)\n",
        "    # We add a dimension to transform our array into a \"batch\"\n",
        "    # of size (1, 299, 299, 3)\n",
        "    array = np.expand_dims(array, axis=0)\n",
        "    return array\n",
        "\n",
        "\n",
        "def make_gradcam_heatmap(\n",
        "    img_array, model, last_conv_layer, classifier_layer\n",
        "):\n",
        "    # First, we create a model that maps the input image to the activations\n",
        "    # of the last conv layer\n",
        "    \n",
        "    last_conv_layer_model = keras.Model(model.inputs, last_conv_layer.output)\n",
        "\n",
        "    # Second, we create a model that maps the activations of the last conv\n",
        "    # layer to the final class predictions\n",
        "    classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])\n",
        "    x = classifier_input\n",
        "    x = keras.layers.Flatten()(x)\n",
        "    x = classifier_layer(x)\n",
        "    classifier_model = keras.Model(classifier_input, x)\n",
        "\n",
        "    # Then, we compute the gradient of the top predicted class for our input image\n",
        "    # with respect to the activations of the last conv layer\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Compute activations of the last conv layer and make the tape watch it\n",
        "        last_conv_layer_output = last_conv_layer_model(img_array)\n",
        "        tape.watch(last_conv_layer_output)\n",
        "        # Compute class predictions\n",
        "        preds = classifier_model(last_conv_layer_output)\n",
        "        top_pred_index = tf.argmax(preds[0])\n",
        "        top_class_channel = preds[:, top_pred_index]\n",
        "\n",
        "    # This is the gradient of the top predicted class with regard to\n",
        "    # the output feature map of the last conv layer\n",
        "    grads = tape.gradient(top_class_channel, last_conv_layer_output)\n",
        "\n",
        "    # This is a vector where each entry is the mean intensity of the gradient\n",
        "    # over a specific feature map channel\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    # We multiply each channel in the feature map array\n",
        "    # by \"how important this channel is\" with regard to the top predicted class\n",
        "    last_conv_layer_output = last_conv_layer_output.numpy()[0]\n",
        "    pooled_grads = pooled_grads.numpy()\n",
        "    for i in range(pooled_grads.shape[-1]):\n",
        "        last_conv_layer_output[:, :, i] *= pooled_grads[i]\n",
        "\n",
        "    # The channel-wise mean of the resulting feature map\n",
        "    # is our heatmap of class activation\n",
        "    heatmap = np.mean(last_conv_layer_output, axis=-1)\n",
        "\n",
        "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
        "    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n",
        "    return heatmap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1xQw3Y5ka1T"
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "last_conv_layer = model.layers[-4]\n",
        "classifier_layer = model.layers[-1]\n",
        "\n",
        "for idx, (images, output) in enumerate(test_generator):\n",
        "\n",
        "    if idx == 1:\n",
        "        break\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        img = images[i] * 255\n",
        "\n",
        "        image = np.expand_dims(images[i], axis=0)\n",
        "\n",
        "        preds = model.predict(image)\n",
        "\n",
        "        actual_value = np.argmax(output[i], axis=0)\n",
        "\n",
        "        predicted_value = np.argmax(preds, axis=1)[0]\n",
        "\n",
        "        confidence = preds[0][predicted_value] * 100\n",
        "\n",
        "        # Generate class activation heatmap\n",
        "        heatmap = make_gradcam_heatmap(\n",
        "            image, model, last_conv_layer, classifier_layer\n",
        "        )\n",
        "\n",
        "\n",
        "        # We rescale heatmap to a range 0-255\n",
        "        heatmap = np.uint8(255 * heatmap)\n",
        "\n",
        "        # We use jet colormap to colorize heatmap\n",
        "        jet = cm.get_cmap(\"jet\")\n",
        "\n",
        "        # We use RGB values of the colormap\n",
        "        jet_colors = jet(np.arange(256))[:, :3]\n",
        "        jet_heatmap = jet_colors[heatmap]\n",
        "\n",
        "        # We create an image with RGB colorized heatmap\n",
        "        jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n",
        "        jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
        "        jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n",
        "\n",
        "        # Superimpose the heatmap on original image\n",
        "        superimposed_img = jet_heatmap * 0.6 + img\n",
        "        superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n",
        "\n",
        "        plt.figure(figsize=(5,5))\n",
        "        plt.imshow(superimposed_img)\n",
        "        plt.show()\n",
        "\n",
        "        print(\"Confidence\", confidence, \"%\")\n",
        "        print(\"Actual_value\", idx_to_attribute[int(actual_value)])\n",
        "        print(\"Predicted value\", idx_to_attribute[int(predicted_value)])\n",
        "    \n",
        "        \n",
        "    idx += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9wZ1-hck-VE"
      },
      "source": [
        "#### Visualise detection results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o1dzdsomJD9"
      },
      "source": [
        "batch_size = 1\n",
        "\n",
        "test_generator = generator.flow_from_directory( os.path.join(new_dataset, 'test'), \n",
        "                                                target_size=(224, 224), \n",
        "                                                batch_size=batch_size,\n",
        "                                                class_mode='categorical') \n",
        "disp_images = []\n",
        "\n",
        "pos_idx = 0\n",
        "neg_idx = 0\n",
        "\n",
        "for image, actual in test_generator:\n",
        "\n",
        "    output = np.argmax(actual, axis=1)[0]\n",
        "    \n",
        "    if output == 0 and pos_idx < 3:\n",
        "        pos_idx += 1\n",
        "        disp_images.append(image[0, ...])\n",
        "\n",
        "    \n",
        "    if output == 1 and neg_idx <3:\n",
        "        neg_idx += 1\n",
        "        disp_images.append(image[0, ...])\n",
        "\n",
        "    if neg_idx>=3 and pos_idx>=3:\n",
        "        break\n",
        "\n",
        "disp_images = np.array(disp_images)\n",
        "print(disp_images.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_-Ly_pMmBAx"
      },
      "source": [
        "#takes in 6 images and activation maps, reutrns a 3x2 grid of images with box highlighting   \n",
        "def plot_maps(images, maps, k, dims):\n",
        "    map_size=dims[0]\n",
        "    r_size = dims[1]\n",
        "    r_stride = dims[2]\n",
        "    colors = ['#ff0000', '#ff0080', '#ff00ff', '#8000ff', '#0080ff', '#00ffff', '#00ff80']\n",
        "              \n",
        "    fig, ax = plt.subplots(2, 3, figsize=(20,20))\n",
        "    for j , (img, mask) in enumerate(zip(images, maps)):\n",
        "       \n",
        "        answer = np.argmax(model.predict(np.expand_dims(img, axis=0)), axis=1)[0]\n",
        "        answer = idx_to_attribute[answer]\n",
        "\n",
        "        mask = mask.flatten()\n",
        "        j_a = (j-j%3)//3\n",
        "        j_b = j%3\n",
        "        img = np.asarray(img[:,:,0],dtype=np.float32)/255\n",
        "        ax[j_a][j_b].imshow(img,cmap=plt.get_cmap('gray'))\n",
        "        \n",
        "        for i in range(k):\n",
        "            a_max = np.argmax(mask)\n",
        "            x_region = a_max%map_size#note, numpy addresses work on arr[y, x, z], compared to image coordinates\n",
        "            y_region = (a_max-(a_max%map_size))//map_size\n",
        "            prob = mask[a_max]\n",
        "            if prob<0.5:break\n",
        "            x_pixel = r_stride*(x_region)\n",
        "            y_pixel = r_stride*(y_region)\n",
        "\n",
        "            #rectangle expects bottom left coordinates. We've generated Top Right\n",
        "            rectangle = mplot.patches.Rectangle((x_pixel, y_pixel), r_size, r_size, edgecolor=colors[i-1],facecolor=\"none\")\n",
        "            ax[j_a][j_b].add_patch(rectangle)\n",
        "            \n",
        "            font = {'color':colors[i-1], 'size':20}\n",
        "            ax[j_a][j_b].text(x_pixel,y_pixel,s=\"{0:.3f}\".format(prob), fontdict=font)\n",
        "            ax[j_a][j_b].text(200,200, s=answer)\n",
        "            mask[a_max]= 0#to help get the next most maximum\n",
        "    plt.subplots_adjust(wspace=0, hspace=0)\n",
        "    plt.show()   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc-ruMqwlDfH"
      },
      "source": [
        "map_model = keras.models.Model(inputs=[model.input], \n",
        "                               outputs=[model.layers[-3].output])\n",
        "\n",
        "final_layer = model.layers[-1]\n",
        "\n",
        "pred_maps = map_model.predict(disp_images)\n",
        "\n",
        "plot_maps(disp_images, pred_maps, 10,  (11, 32, 20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGZyKHQDL9V5"
      },
      "source": [
        "## STEEL DEFECT DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpueROgeMAmA"
      },
      "source": [
        "### Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9gef8BJL_73"
      },
      "source": [
        "import os\n",
        "import tarfile\n",
        "import shutil\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm.notebook import tqdm \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "dataset_dir = '/content/drive/MyDrive/Major Project/Datasets/datasets'\n",
        "\n",
        "model_dir = '/content/drive/MyDrive/Major Project/Models/V2'\n",
        "\n",
        "if not os.path.exists(model_dir):\n",
        "    os.mkdir(model_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hk7FthpMV4Kf"
      },
      "source": [
        "#### Balancing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfhCbUhoPb79"
      },
      "source": [
        "dataset = os.path.join(dataset_dir, 'steel_defect_cls_products_balanced')\n",
        "\n",
        "count_dict = {}\n",
        "\n",
        "train_dataset = os.path.join(dataset, 'train')\n",
        "\n",
        "for category in os.listdir(train_dataset):\n",
        "\n",
        "    category_path = os.path.join(train_dataset, category)\n",
        "\n",
        "    count_dict[category] = len(os.listdir(category_path))\n",
        "\n",
        "print(count_dict)\n",
        "\"\"\"\n",
        "cutoff = 0\n",
        "\n",
        "if len(count_dict) == 2:\n",
        "    cutoff = min(count_dict.values())\n",
        "\n",
        "if len(count_dict) > 2:\n",
        "    cutoff = int(np.median(list(count_dict.values())))\n",
        "\n",
        "print(cutoff)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DRo9lrIOGq_"
      },
      "source": [
        "dataset = os.path.join(dataset_dir, 'steel_defect_cls_products')\n",
        "new_dataset = os.path.join(dataset_dir, 'steel_defect_cls_products_balanced')\n",
        "\n",
        "if not os.path.exists(new_dataset):\n",
        "    os.mkdir(new_dataset)\n",
        "\n",
        "for split in os.listdir(dataset):\n",
        "\n",
        "    print(\" \", split)\n",
        "    split_path = os.path.join(dataset, split)\n",
        "    new_split_path = os.path.join(new_dataset, split)\n",
        "\n",
        "    if not os.path.exists(new_split_path):\n",
        "        os.mkdir(new_split_path)\n",
        "\n",
        "    for category in os.listdir(split_path):\n",
        "\n",
        "        print(\"     \", category)\n",
        "        \n",
        "        category_path = os.path.join(split_path, category)\n",
        "        new_category_path = os.path.join(new_split_path, category)\n",
        "\n",
        "        if not os.path.exists(new_category_path):\n",
        "            os.mkdir(new_category_path)\n",
        "        \n",
        "        images = os.listdir(category_path)\n",
        "\n",
        "        for i in range(2):\n",
        "            random.shuffle(images)\n",
        "\n",
        "        if len(images) > cutoff:\n",
        "\n",
        "            images = images[:cutoff]\n",
        "        \n",
        "        for image in tqdm(images):\n",
        "\n",
        "            source = os.path.join(category_path, image)\n",
        "            destination = os.path.join(new_category_path, image)\n",
        "\n",
        "            shutil.copyfile(source, destination)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iGktxr-V_YW"
      },
      "source": [
        "#### Creating data generators for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2zri7_jOGvY"
      },
      "source": [
        "import keras\n",
        "\n",
        "new_dataset = os.path.join(dataset_dir, 'steel_defect_cls_products_balanced')\n",
        "\n",
        "generator = keras.preprocessing.image.ImageDataGenerator(\n",
        "                            width_shift_range=0.1,\n",
        "                            height_shift_range=0.1,\n",
        "                            rescale=1./255,\n",
        "                            fill_mode=\"constant\",\n",
        "                            cval=0.0,\n",
        "                            horizontal_flip=True,\n",
        "                            vertical_flip=True,\n",
        "                            validation_split=0.1,\n",
        "                            dtype='float32',\n",
        "                        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXR4VHE7OHN2"
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_generator = generator.flow_from_directory( os.path.join(new_dataset, 'train'), \n",
        "                                                target_size=(200, 700), \n",
        "                                                batch_size=batch_size,\n",
        "                                                class_mode='binary',\n",
        "                                                subset='training') \n",
        "                                            \n",
        "val_generator = generator.flow_from_directory( os.path.join(new_dataset, 'train'), \n",
        "                                                target_size=(200, 700), \n",
        "                                                batch_size=batch_size,\n",
        "                                                class_mode='categorical',\n",
        "                                                subset='validation') \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDtj9vxOYioT"
      },
      "source": [
        "attribute_to_idx = train_generator.class_indices\n",
        "idx_to_attribute = {value:key for key,value in attribute_to_idx.items()}\n",
        "\n",
        "print(attribute_to_idx)\n",
        "print(idx_to_attribute)\n",
        "\n",
        "number_of_attributes = len(idx_to_attribute)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eCw7-iLXUjV"
      },
      "source": [
        "a = next(train_generator)\n",
        "\n",
        "for img in a[1][:10]:\n",
        "    print(img)\n",
        "    plt.figure(figsize=(15,10))\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzNAzL5vX7dJ"
      },
      "source": [
        "### Building the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVIvM5wQX7dK"
      },
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "\n",
        "tf.config.run_functions_eagerly(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XgvLjNHX7dL"
      },
      "source": [
        "def identity_block(X, f, filters, stage, block):\n",
        "    \"\"\"\n",
        "    Implementation of the identity block as defined in Figure 3\n",
        "\n",
        "    Arguments:\n",
        "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
        "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
        "    stage -- integer, used to name the layers, depending on their position in the network\n",
        "    block -- string/character, used to name the layers, depending on their position in the network\n",
        "\n",
        "    Returns:\n",
        "    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n",
        "    \"\"\"\n",
        "    policy = tf.keras.mixed_precision.experimental.Policy(\n",
        "        'mixed_float16') \n",
        "    \n",
        "    # defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "\n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "\n",
        "    # Save the input value. You'll need this later to add back to the main path.\n",
        "    X_shortcut = X\n",
        "\n",
        "    # First component of main path\n",
        "    X = keras.layers.Conv2D(filters=F1, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2a',\n",
        "                            kernel_initializer=keras.initializers.glorot_uniform(seed=0), dtype=policy)(X)\n",
        "    X = keras.layers.BatchNormalization(axis=3, name=bn_name_base + '2a', dtype=policy)(X)\n",
        "    X = keras.layers.Activation('relu')(X)\n",
        "\n",
        "    # Second component of main path (≈3 lines)\n",
        "    X = keras.layers.Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same', name=conv_name_base + '2b',\n",
        "                            kernel_initializer=keras.initializers.glorot_uniform(seed=0), dtype=policy)(X)\n",
        "    X = keras.layers.BatchNormalization(axis=3, name=bn_name_base + '2b', dtype=policy)(X)\n",
        "    X = keras.layers.Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path (≈2 lines)\n",
        "    X = keras.layers.Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2c',\n",
        "                            kernel_initializer=keras.initializers.glorot_uniform(seed=0), dtype=policy)(X)\n",
        "    X = keras.layers.BatchNormalization(axis=3, name=bn_name_base + '2c', dtype=policy)(X)\n",
        "\n",
        "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
        "    X = keras.layers.Add()([X, X_shortcut])\n",
        "    X = keras.layers.Activation('relu')(X)\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "def convolutional_block(X, f, filters, stage, block, s=2):\n",
        "    \"\"\"\n",
        "    Implementation of the convolutional block as defined in Figure 4\n",
        "\n",
        "    Arguments:\n",
        "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
        "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
        "    stage -- integer, used to name the layers, depending on their position in the network\n",
        "    block -- string/character, used to name the layers, depending on their position in the network\n",
        "    s -- Integer, specifying the stride to be used\n",
        "\n",
        "    Returns:\n",
        "    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n",
        "    \"\"\"\n",
        "\n",
        "    policy = tf.keras.mixed_precision.experimental.Policy(\n",
        "        'mixed_float16') \n",
        "\n",
        "    # defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "\n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "\n",
        "    # Save the input value\n",
        "    X_shortcut = X\n",
        "\n",
        "    ##### MAIN PATH #####\n",
        "    # First component of main path\n",
        "    X = keras.layers.Conv2D(F1, (1, 1), strides=(s, s), name=conv_name_base + '2a', \n",
        "                            kernel_initializer=keras.initializers.glorot_uniform(seed=0), dtype=policy)(X)\n",
        "    X = keras.layers.BatchNormalization(axis=3, name=bn_name_base + '2a', dtype=policy)(X)\n",
        "    X = keras.layers.Activation('relu')(X)\n",
        "\n",
        "    # Second component of main path (≈3 lines)\n",
        "    X = keras.layers.Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same', name=conv_name_base + '2b',\n",
        "                            kernel_initializer=keras.initializers.glorot_uniform(seed=0), dtype=policy)(X)\n",
        "    X = keras.layers.BatchNormalization(axis=3, name=bn_name_base + '2b', dtype=policy)(X)\n",
        "    X = keras.layers.Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path (≈2 lines)\n",
        "    X = keras.layers.Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2c',\n",
        "                            kernel_initializer=keras.initializers.glorot_uniform(seed=0), dtype=policy)(X)\n",
        "    X = keras.layers.BatchNormalization(axis=3, name=bn_name_base + '2c', dtype=policy)(X)\n",
        "\n",
        "    ##### SHORTCUT PATH #### (≈2 lines)\n",
        "    X_shortcut = keras.layers.Conv2D(filters=F3, kernel_size=(1, 1), strides=(s, s), padding='valid', name=conv_name_base + '1',\n",
        "                                     kernel_initializer=keras.initializers.glorot_uniform(seed=0), dtype=policy)(X_shortcut)\n",
        "    X_shortcut = keras.layers.BatchNormalization(axis=3, name=bn_name_base + '1', dtype=policy)(X_shortcut)\n",
        "\n",
        "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
        "    X = keras.layers.Add()([X, X_shortcut])\n",
        "    X = keras.layers.Activation('relu')(X)\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "def build_model(input_shape, out_type, number_of_attributes):\n",
        "\n",
        "    policy = tf.keras.mixed_precision.experimental.Policy(\n",
        "        'mixed_float16')  # sets values to be float16 for nvidia 2000,3000 series GPUs, plus others im sure\n",
        "\n",
        "    # Define the input as a tensor with shape input_shape\n",
        "    X_input = keras.layers.Input(input_shape)\n",
        "\n",
        "    # Zero-Padding\n",
        "    X = keras.layers.ZeroPadding2D((3, 3), dtype=policy)(X_input)\n",
        "\n",
        "    # Stage 1\n",
        "    X = keras.layers.Conv2D(64, (7, 7), strides=(2, 2), name='conv1',\n",
        "                            kernel_initializer=keras.initializers.glorot_uniform(seed=0), dtype=policy)(X)\n",
        "    X = keras.layers.BatchNormalization(axis=3, name='bn_conv1', dtype=policy)(X)\n",
        "    X = keras.layers.Activation('relu')(X)\n",
        "    X = keras.layers.MaxPooling2D((3, 3), strides=(2, 2), dtype=policy)(X)\n",
        "\n",
        "    # Stage 2\n",
        "    X = convolutional_block(X, f=3, filters=[16, 16, 32], stage=2, block='a', s=1)\n",
        "    X = identity_block(X, 3, [16, 16, 32], stage=2, block='b')\n",
        "    X = identity_block(X, 3, [16, 16, 32], stage=2, block='c')\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # Stage 3 (≈4 lines)\n",
        "    X = convolutional_block(X, f=3, filters=[32, 32, 64], stage=3, block='a', s=2)\n",
        "    X = identity_block(X, 3, [32, 32, 64], stage=3, block='b')\n",
        "    X = identity_block(X, 3, [32, 32, 64], stage=3, block='c')\n",
        "    X = identity_block(X, 3, [32, 32, 64], stage=3, block='d')\n",
        "\n",
        "    # Stage 4 (≈6 lines)\n",
        "    X = convolutional_block(X, f=3, filters=[64, 64, 128], stage=4, block='a', s=2)\n",
        "    X = identity_block(X, 3, [64, 64, 128], stage=4, block='b')\n",
        "    X = identity_block(X, 3, [64, 64, 128], stage=4, block='c')\n",
        "    X = identity_block(X, 3, [64, 64, 128], stage=4, block='d')\n",
        "    X = identity_block(X, 3, [64, 64, 128], stage=4, block='e')\n",
        "    X = identity_block(X, 3, [64, 64, 128], stage=4, block='f')\n",
        "\n",
        "    # Stage 5 (≈3 lines)\n",
        "    X = convolutional_block(X, f=3, filters=[128, 128, 256], stage=5, block='a', s=2)\n",
        "    X = identity_block(X, 3, [128, 128, 256], stage=5, block='b')\n",
        "    X = identity_block(X, 3, [128, 128, 256], stage=5, block='c')\n",
        "\n",
        "    # AVGPOOL (≈1 line). Use \"X = AveragePooling2D(...)(X)\"\n",
        "    #X = keras.layers.AveragePooling2D((2, 2), name=\"avg_pool\")(X)\n",
        "\n",
        "    X = keras.layers.Conv2D(16, 1, activation=\"selu\", kernel_initializer=\"lecun_normal\", dtype=policy)(X)\n",
        "    #new\n",
        "    X = keras.layers.Conv2DTranspose(1, 3, activation='linear', dtype=policy)(X)\n",
        "    #new\n",
        "    X = keras.layers.Conv2DTranspose(1, 3, activation='linear', dtype=policy)(X)\n",
        "    #X = keras.layers.Conv2D(1, 1, activation='linear')(X)\n",
        "    X_map = keras.activations.sigmoid(X)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # output layer\n",
        "\n",
        "    # new - changed X to X_map\n",
        "    X = keras.layers.Flatten()(X_map)\n",
        "    y = keras.layers.Dense(number_of_attributes, activation='sigmoid', name='fc' + str(number_of_attributes),\n",
        "                           kernel_initializer=keras.initializers.glorot_uniform(seed=0), dtype=policy)(X)\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
        "    metrics = [\n",
        "        keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
        "        keras.metrics.Precision(name='precision'),\n",
        "        keras.metrics.Recall(name='recall'),\n",
        "        keras.metrics.AUC(name='auc'),\n",
        "    ]\n",
        "\n",
        "    if out_type == \"classify\":\n",
        "        model = keras.models.Model(inputs=X_input, outputs=y)\n",
        "        model.compile(optimizer=optimizer,\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=metrics)\n",
        "        \n",
        "    if out_type == \"map\":\n",
        "        model = keras.models.Model(inputs=X_input, outputs=X_map)\n",
        "        model.compile(optimizer=optimizer,\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=metrics)\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AP8Y1J7QMD8W"
      },
      "source": [
        "model = build_model((200,700,3), \"classify\", number_of_attributes)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM1wRHbJY2uh"
      },
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5mt5mWRY5A0"
      },
      "source": [
        "epochs = 50\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', patience=5, min_lr = 1e-8, factor=0.5, verbose=1)\n",
        "\n",
        "history = model.fit(train_generator, \n",
        "                    epochs= epochs,\n",
        "                    validation_data = val_generator,\n",
        "                    verbose=1,\n",
        "                    callbacks=[reduce_lr],\n",
        "                   )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cju-jLfW6-WA"
      },
      "source": [
        "model.save(os.path.join(model_dir, 'steel_V1_100eps.h5'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7M_jCsrS6-wO"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['lr'])\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['precision'])\n",
        "plt.plot(history.history['val_precision'])\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['recall'])\n",
        "plt.plot(history.history['val_recall'])\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['auc'])\n",
        "\n",
        "plt.plot(history.history['val_auc'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obfs-QvMaxxj"
      },
      "source": [
        "### Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPcCWbEMhTGz"
      },
      "source": [
        "model = keras.models.load_model(os.path.join(model_dir, 'steel_V1_100eps.h5'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JRX8cD1a1Bw"
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "test_generator = generator.flow_from_directory( os.path.join(new_dataset, 'test'), \n",
        "                                                target_size=(200, 800), \n",
        "                                                batch_size=batch_size,\n",
        "                                                class_mode='categorical') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHUbyn7Ia6jP"
      },
      "source": [
        "scores = model.evaluate_generator(test_generator, verbose=1)\n",
        "scores_keys = ['loss', 'accuracy', 'precision', 'recall', 'auc']\n",
        "\n",
        "for key,score in zip(scores_keys, scores):\n",
        "\n",
        "    print(key, ':', score)\n",
        "\n",
        "\"\"\"\n",
        "100 eps\n",
        "\n",
        "loss : 0.6535497307777405\n",
        "accuracy : 0.7710843086242676\n",
        "precision : 0.7991266250610352\n",
        "recall : 0.7349397540092468\n",
        "auc : 0.9266652464866638\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}