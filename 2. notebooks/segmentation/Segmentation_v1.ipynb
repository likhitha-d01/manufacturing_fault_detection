{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Segmentation_v1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "lbxmdITkL2TX",
        "nGZyKHQDL9V5",
        "_ZzyozMIjPTQ",
        "frdg4R1BjPTf",
        "YSWVQhQHjQxW"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBslyrAnjBht"
      },
      "source": [
        "!nvidia-smi "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZytusM1GPCJ"
      },
      "source": [
        "# Segmentation model - Version 1\n",
        "\n",
        "The classification backbone of the model is VGG-16 network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvpGmGJE3Nj_"
      },
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "import keras.backend as K "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soddmJcwU0Sl"
      },
      "source": [
        "filename = 'casting_dataset.tar.xz'\n",
        "tar_file = os.path.join(dataset_dir, filename)\n",
        "\n",
        "my_tar = tarfile.open(tar_file)\n",
        "my_tar.extractall(dataset_dir) # specify which folder to extract to\n",
        "my_tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYv62k-uGQUV"
      },
      "source": [
        "## Building model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2E_4VPv4afX"
      },
      "source": [
        "def build_model(optimizer, learning_rate, out_type):\n",
        "\n",
        "    policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')#sets values to be float16 for nvidia 2000,3000 series GPUs, plus others im sure\n",
        "    \n",
        "    input_img = keras.layers.Input(shape=(224, 224, 3))\n",
        "  \n",
        "    x = keras.layers.Conv2D(8, 3,padding='valid',activation='selu', kernel_initializer='lecun_normal',dtype=policy)(input_img)\n",
        "    x = keras.layers.Conv2D(8, 3,padding='valid',activation='selu', kernel_initializer='lecun_normal',dtype=policy)(x)\n",
        "    x = keras.layers.MaxPooling2D(2)(x)\n",
        "    x = keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    x = keras.layers.Conv2D(16, 3,padding='valid',activation='selu', kernel_initializer='lecun_normal',dtype=policy)(x)\n",
        "    x = keras.layers.Conv2D(16, 3,padding='valid',activation='selu', kernel_initializer='lecun_normal',dtype=policy)(x)\n",
        "    x = keras.layers.MaxPooling2D(2)(x)\n",
        "    x = keras.layers.BatchNormalization()(x)\n",
        "    \n",
        "    x = keras.layers.Conv2D(16, 2,padding='valid',activation='selu', kernel_initializer='lecun_normal',dtype=policy)(x)\n",
        "    x = keras.layers.Conv2D(16, 3,padding='valid',activation='selu', kernel_initializer='lecun_normal',dtype=policy)(x)\n",
        "    x = keras.layers.MaxPooling2D(2)(x)\n",
        "    x = keras.layers.BatchNormalization()(x)\n",
        "    \n",
        "    x = keras.layers.Conv2D(32, 2,padding='valid',activation='selu', kernel_initializer='lecun_normal',dtype=policy)(x)\n",
        "    x = keras.layers.Conv2D(32, 3,padding='valid',activation='selu', kernel_initializer='lecun_normal',dtype=policy)(x)\n",
        "    x = keras.layers.MaxPooling2D(2)(x)\n",
        "    x = keras.layers.BatchNormalization()(x)    \n",
        "\n",
        "    x = keras.layers.Conv2D(16, 1, activation=\"selu\",kernel_initializer=\"lecun_normal\",dtype=policy)(x)\n",
        "    x = keras.layers.Conv2D(1, 1, activation='linear')(x)\n",
        "    x_map = keras.activations.sigmoid(x)\n",
        "\n",
        "    y = keras.layers.GlobalMaxPooling2D()(x_map)\n",
        "    \n",
        "    if optimizer == \"SGD\":\n",
        "        #looks like pretty good but noisy results with no momentum, lets check 0.9...\n",
        "        optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
        "    if optimizer == \"RMSprop\":\n",
        "        optimizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "    if optimizer == \"Adam\":\n",
        "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "    if out_type == \"classify\":\n",
        "        model = keras.models.Model(inputs=input_img, outputs=y)\n",
        "    if out_type == \"map\":\n",
        "        model = keras.models.Model(inputs=input_img, outputs=x_map)\n",
        "    model.compile(loss=\"binary_crossentropy\",optimizer=optimizer, metrics=['accuracy','AUC'])\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDbFkajCHDgG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d544aced-b947-4759-8336-f2f8077924f8"
      },
      "source": [
        "map_model = build_model(\"Adam\", 1e-3, \"map\")\n",
        "model = build_model(\"Adam\", 1e-3, \"classify\")\n",
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
            "Your GPU may run slowly with dtype policy mixed_float16 because it does not have compute capability of at least 7.0. Your GPU:\n",
            "  Tesla P100-PCIE-16GB, compute capability 6.0\n",
            "See https://developer.nvidia.com/cuda-gpus for a list of GPUs and their compute capabilities.\n",
            "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale.py:56: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 222, 222, 8)       224       \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 220, 220, 8)       584       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 110, 110, 8)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 110, 110, 8)       32        \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 108, 108, 16)      1168      \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 106, 106, 16)      2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 53, 53, 16)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 53, 53, 16)        64        \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 52, 52, 16)        1040      \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 50, 50, 16)        2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 25, 25, 16)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 25, 25, 16)        64        \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 24, 24, 32)        2080      \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 22, 22, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 11, 11, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 11, 11, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 11, 11, 16)        528       \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 11, 11, 1)         17        \n",
            "_________________________________________________________________\n",
            "tf.math.sigmoid_1 (TFOpLambd (None, 11, 11, 1)         0         \n",
            "_________________________________________________________________\n",
            "global_max_pooling2d_1 (Glob (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 19,817\n",
            "Trainable params: 19,673\n",
            "Non-trainable params: 144\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ5iCdZVKsaN"
      },
      "source": [
        "## CASTING DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDYvlKbxKxWq"
      },
      "source": [
        "### Pre-processing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZIlPBW4KwpV"
      },
      "source": [
        "import os\n",
        "import tarfile\n",
        "import shutil\n",
        "import numpy as np\n",
        "\n",
        "dataset_dir = '/content/drive/MyDrive/Major Project/Datasets/datasets'\n",
        "\n",
        "model_dir = '/content/drive/MyDrive/Major Project/Models/V1'\n",
        "\n",
        "if not os.path.exists(model_dir):\n",
        "    os.mkdir(model_dir)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpDXSDujh2Ql"
      },
      "source": [
        "dataset = os.path.join(dataset_dir, 'casting_dataset')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zpd2cxHmU0VY"
      },
      "source": [
        "generator = keras.preprocessing.image.ImageDataGenerator(\n",
        "                            rotation_range=10,\n",
        "                            width_shift_range=0.2,\n",
        "                            height_shift_range=0.2,\n",
        "                            brightness_range = [0.5,1.0],\n",
        "                            zoom_range=0.1,\n",
        "                            rescale=1./255,\n",
        "                            fill_mode=\"nearest\",\n",
        "                            cval=1.0,\n",
        "                            horizontal_flip=True,\n",
        "                            vertical_flip=True,\n",
        "                            validation_split=0.1,\n",
        "                            dtype='float64',\n",
        "                        )\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivTMPvTVdpJo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6b4fb13-0939-4716-b331-7b536fcdb1c5"
      },
      "source": [
        "print(len(os.listdir(os.path.join(dataset, 'train', 'def_front'))))\n",
        "print(len(os.listdir(os.path.join(dataset, 'train', 'ok_front'))))\n",
        "print(len(os.listdir(os.path.join(dataset, 'test', 'def_front'))))\n",
        "print(len(os.listdir(os.path.join(dataset, 'test', 'ok_front'))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3753\n",
            "2734\n",
            "421\n",
            "313\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mkka89-hLb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57c54215-82bb-4eb1-da43-fbc26a4d2f79"
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_generator = generator.flow_from_directory( os.path.join(dataset, 'train'), \n",
        "                                                target_size=(224, 224), \n",
        "                                                batch_size=batch_size,\n",
        "                                                class_mode='binary',\n",
        "                                                subset='training',\n",
        "                                                shuffle=True) \n",
        "                                            \n",
        "val_generator = generator.flow_from_directory( os.path.join(dataset, 'train'), \n",
        "                                                target_size=(224, 224), \n",
        "                                                batch_size=batch_size,\n",
        "                                                class_mode='binary',\n",
        "                                                subset='validation',\n",
        "                                                shuffle=True) \n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 5839 images belonging to 2 classes.\n",
            "Found 648 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqneUq65LCM0"
      },
      "source": [
        "### Training model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okBDv1VnLDmm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2917e66-947d-4d21-c091-e67212848dc5"
      },
      "source": [
        "epochs = 100\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', patience=5, min_lr = 1e-7, factor=0.5, verbose=1)\n",
        "\n",
        "history = model.fit(train_generator, \n",
        "                    epochs= epochs,\n",
        "                    validation_data = val_generator,\n",
        "                    verbose=1,\n",
        "                    callbacks=[reduce_lr]) "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "92/92 [==============================] - 6643s 72s/step - loss: 0.8624 - accuracy: 0.5032 - auc: 0.5456 - val_loss: 0.6676 - val_accuracy: 0.6296 - val_auc: 0.6420\n",
            "Epoch 2/100\n",
            "92/92 [==============================] - 96s 1s/step - loss: 0.6562 - accuracy: 0.6143 - auc: 0.6347 - val_loss: 0.6867 - val_accuracy: 0.5818 - val_auc: 0.6391\n",
            "Epoch 3/100\n",
            "92/92 [==============================] - 95s 1s/step - loss: 0.6504 - accuracy: 0.6097 - auc: 0.6371 - val_loss: 0.6448 - val_accuracy: 0.6265 - val_auc: 0.6786\n",
            "Epoch 4/100\n",
            "92/92 [==============================] - 95s 1s/step - loss: 0.6374 - accuracy: 0.6398 - auc: 0.6641 - val_loss: 0.7782 - val_accuracy: 0.4290 - val_auc: 0.5970\n",
            "Epoch 5/100\n",
            "92/92 [==============================] - 93s 1s/step - loss: 0.6257 - accuracy: 0.6566 - auc: 0.6826 - val_loss: 0.6548 - val_accuracy: 0.6034 - val_auc: 0.6955\n",
            "Epoch 6/100\n",
            "92/92 [==============================] - 96s 1s/step - loss: 0.6305 - accuracy: 0.6499 - auc: 0.6781 - val_loss: 0.6276 - val_accuracy: 0.6404 - val_auc: 0.6878\n",
            "Epoch 7/100\n",
            "92/92 [==============================] - 94s 1s/step - loss: 0.6199 - accuracy: 0.6655 - auc: 0.6911 - val_loss: 0.6404 - val_accuracy: 0.6590 - val_auc: 0.6877\n",
            "Epoch 8/100\n",
            "92/92 [==============================] - 94s 1s/step - loss: 0.6154 - accuracy: 0.6629 - auc: 0.6961 - val_loss: 0.6603 - val_accuracy: 0.6204 - val_auc: 0.6503\n",
            "Epoch 9/100\n",
            "92/92 [==============================] - 95s 1s/step - loss: 0.6066 - accuracy: 0.6697 - auc: 0.7170 - val_loss: 0.7844 - val_accuracy: 0.5664 - val_auc: 0.6493\n",
            "Epoch 10/100\n",
            "92/92 [==============================] - 95s 1s/step - loss: 0.6095 - accuracy: 0.6648 - auc: 0.7114 - val_loss: 0.7373 - val_accuracy: 0.5864 - val_auc: 0.6105\n",
            "Epoch 11/100\n",
            "92/92 [==============================] - 94s 1s/step - loss: 0.5984 - accuracy: 0.6780 - auc: 0.7298 - val_loss: 0.6370 - val_accuracy: 0.7006 - val_auc: 0.7333\n",
            "Epoch 12/100\n",
            "92/92 [==============================] - 95s 1s/step - loss: 0.5890 - accuracy: 0.6800 - auc: 0.7351 - val_loss: 0.7927 - val_accuracy: 0.5710 - val_auc: 0.6768\n",
            "Epoch 13/100\n",
            "92/92 [==============================] - 95s 1s/step - loss: 0.5840 - accuracy: 0.6943 - auc: 0.7487 - val_loss: 0.7529 - val_accuracy: 0.5849 - val_auc: 0.6271\n",
            "Epoch 14/100\n",
            "92/92 [==============================] - 96s 1s/step - loss: 0.5645 - accuracy: 0.7134 - auc: 0.7689 - val_loss: 0.7840 - val_accuracy: 0.5787 - val_auc: 0.6611\n",
            "Epoch 15/100\n",
            "92/92 [==============================] - 96s 1s/step - loss: 0.5268 - accuracy: 0.7480 - auc: 0.8190 - val_loss: 0.5624 - val_accuracy: 0.7130 - val_auc: 0.8001\n",
            "Epoch 16/100\n",
            "92/92 [==============================] - 95s 1s/step - loss: 0.4784 - accuracy: 0.7832 - auc: 0.8604 - val_loss: 0.5482 - val_accuracy: 0.7176 - val_auc: 0.7963\n",
            "Epoch 17/100\n",
            "92/92 [==============================] - 95s 1s/step - loss: 0.4608 - accuracy: 0.7958 - auc: 0.8656 - val_loss: 0.6289 - val_accuracy: 0.6713 - val_auc: 0.8378\n",
            "Epoch 18/100\n",
            "92/92 [==============================] - 95s 1s/step - loss: 0.4032 - accuracy: 0.8309 - auc: 0.8999 - val_loss: 0.5398 - val_accuracy: 0.7870 - val_auc: 0.8784\n",
            "Epoch 19/100\n",
            "92/92 [==============================] - 94s 1s/step - loss: 0.4008 - accuracy: 0.8237 - auc: 0.8977 - val_loss: 1.1532 - val_accuracy: 0.5802 - val_auc: 0.6468\n",
            "Epoch 20/100\n",
            "92/92 [==============================] - 95s 1s/step - loss: 0.3714 - accuracy: 0.8367 - auc: 0.9142 - val_loss: 0.6112 - val_accuracy: 0.7269 - val_auc: 0.8243\n",
            "Epoch 21/100\n",
            "92/92 [==============================] - 95s 1s/step - loss: 0.3190 - accuracy: 0.8673 - auc: 0.9393 - val_loss: 0.3101 - val_accuracy: 0.8827 - val_auc: 0.9455\n",
            "Epoch 22/100\n",
            "92/92 [==============================] - 96s 1s/step - loss: 0.3347 - accuracy: 0.8528 - auc: 0.9318 - val_loss: 0.3844 - val_accuracy: 0.8364 - val_auc: 0.9110\n",
            "Epoch 23/100\n",
            "92/92 [==============================] - 95s 1s/step - loss: 0.3036 - accuracy: 0.8762 - auc: 0.9438 - val_loss: 0.4185 - val_accuracy: 0.8302 - val_auc: 0.9391\n",
            "Epoch 24/100\n",
            "92/92 [==============================] - 95s 1s/step - loss: 0.2944 - accuracy: 0.8800 - auc: 0.9472 - val_loss: 0.6056 - val_accuracy: 0.7500 - val_auc: 0.9035\n",
            "Epoch 25/100\n",
            "92/92 [==============================] - 95s 1s/step - loss: 0.2845 - accuracy: 0.8739 - auc: 0.9509 - val_loss: 0.3451 - val_accuracy: 0.8426 - val_auc: 0.9546\n",
            "Epoch 26/100\n",
            "92/92 [==============================] - 95s 1s/step - loss: 0.2591 - accuracy: 0.8942 - auc: 0.9596 - val_loss: 0.2493 - val_accuracy: 0.8889 - val_auc: 0.9727\n",
            "Epoch 27/100\n",
            "92/92 [==============================] - 94s 1s/step - loss: 0.2339 - accuracy: 0.9076 - auc: 0.9668 - val_loss: 1.4187 - val_accuracy: 0.4336 - val_auc: 0.8437\n",
            "Epoch 28/100\n",
            "92/92 [==============================] - 95s 1s/step - loss: 0.2504 - accuracy: 0.8970 - auc: 0.9616 - val_loss: 0.7814 - val_accuracy: 0.6512 - val_auc: 0.9189\n",
            "Epoch 29/100\n",
            "92/92 [==============================] - 95s 1s/step - loss: 0.2363 - accuracy: 0.9036 - auc: 0.9656 - val_loss: 0.3959 - val_accuracy: 0.8194 - val_auc: 0.9046\n",
            "Epoch 30/100\n",
            "92/92 [==============================] - 94s 1s/step - loss: 0.2428 - accuracy: 0.8990 - auc: 0.9635 - val_loss: 0.2212 - val_accuracy: 0.8935 - val_auc: 0.9742\n",
            "Epoch 31/100\n",
            "92/92 [==============================] - 94s 1s/step - loss: 0.2111 - accuracy: 0.9148 - auc: 0.9732 - val_loss: 0.2990 - val_accuracy: 0.8596 - val_auc: 0.9443\n",
            "Epoch 32/100\n",
            "92/92 [==============================] - 95s 1s/step - loss: 0.2078 - accuracy: 0.9140 - auc: 0.9740 - val_loss: 0.3023 - val_accuracy: 0.8920 - val_auc: 0.9488\n",
            "Epoch 33/100\n",
            "92/92 [==============================] - 95s 1s/step - loss: 0.1792 - accuracy: 0.9307 - auc: 0.9799 - val_loss: 0.6046 - val_accuracy: 0.7901 - val_auc: 0.9327\n",
            "Epoch 34/100\n",
            "92/92 [==============================] - 94s 1s/step - loss: 0.1827 - accuracy: 0.9267 - auc: 0.9792 - val_loss: 0.2168 - val_accuracy: 0.9151 - val_auc: 0.9715\n",
            "Epoch 35/100\n",
            "92/92 [==============================] - 95s 1s/step - loss: 0.2059 - accuracy: 0.9164 - auc: 0.9740 - val_loss: 0.3754 - val_accuracy: 0.8488 - val_auc: 0.9561\n",
            "Epoch 36/100\n",
            "92/92 [==============================] - 95s 1s/step - loss: 0.1981 - accuracy: 0.9204 - auc: 0.9754 - val_loss: 0.2632 - val_accuracy: 0.8981 - val_auc: 0.9609\n",
            "Epoch 37/100\n",
            "92/92 [==============================] - 93s 1s/step - loss: 0.2004 - accuracy: 0.9249 - auc: 0.9748 - val_loss: 0.2330 - val_accuracy: 0.9090 - val_auc: 0.9834\n",
            "Epoch 38/100\n",
            "92/92 [==============================] - 95s 1s/step - loss: 0.1700 - accuracy: 0.9381 - auc: 0.9820 - val_loss: 0.1943 - val_accuracy: 0.9151 - val_auc: 0.9819\n",
            "Epoch 39/100\n",
            "92/92 [==============================] - 93s 1s/step - loss: 0.1800 - accuracy: 0.9271 - auc: 0.9801 - val_loss: 0.1732 - val_accuracy: 0.9336 - val_auc: 0.9841\n",
            "Epoch 40/100\n",
            "92/92 [==============================] - 93s 1s/step - loss: 0.1599 - accuracy: 0.9419 - auc: 0.9834 - val_loss: 1.0291 - val_accuracy: 0.6481 - val_auc: 0.8908\n",
            "Epoch 41/100\n",
            "92/92 [==============================] - 93s 1s/step - loss: 0.1512 - accuracy: 0.9418 - auc: 0.9855 - val_loss: 1.3017 - val_accuracy: 0.5617 - val_auc: 0.8824\n",
            "Epoch 42/100\n",
            "92/92 [==============================] - 92s 1s/step - loss: 0.1631 - accuracy: 0.9346 - auc: 0.9832 - val_loss: 0.2232 - val_accuracy: 0.9120 - val_auc: 0.9722\n",
            "Epoch 43/100\n",
            "92/92 [==============================] - 91s 990ms/step - loss: 0.1654 - accuracy: 0.9360 - auc: 0.9830 - val_loss: 0.1869 - val_accuracy: 0.9198 - val_auc: 0.9854\n",
            "Epoch 44/100\n",
            "92/92 [==============================] - 93s 1s/step - loss: 0.1514 - accuracy: 0.9441 - auc: 0.9856 - val_loss: 0.1478 - val_accuracy: 0.9429 - val_auc: 0.9859\n",
            "Epoch 45/100\n",
            "92/92 [==============================] - 92s 1s/step - loss: 0.1512 - accuracy: 0.9478 - auc: 0.9850 - val_loss: 0.1761 - val_accuracy: 0.9383 - val_auc: 0.9850\n",
            "Epoch 46/100\n",
            "92/92 [==============================] - 92s 1s/step - loss: 0.1502 - accuracy: 0.9442 - auc: 0.9861 - val_loss: 0.1674 - val_accuracy: 0.9321 - val_auc: 0.9827\n",
            "Epoch 47/100\n",
            "92/92 [==============================] - 93s 1s/step - loss: 0.1482 - accuracy: 0.9407 - auc: 0.9858 - val_loss: 0.1545 - val_accuracy: 0.9506 - val_auc: 0.9848\n",
            "Epoch 48/100\n",
            "92/92 [==============================] - 94s 1s/step - loss: 0.1478 - accuracy: 0.9414 - auc: 0.9858 - val_loss: 0.2373 - val_accuracy: 0.8935 - val_auc: 0.9767\n",
            "Epoch 49/100\n",
            "92/92 [==============================] - 94s 1s/step - loss: 0.1347 - accuracy: 0.9539 - auc: 0.9880 - val_loss: 0.1550 - val_accuracy: 0.9383 - val_auc: 0.9864\n",
            "Epoch 50/100\n",
            "92/92 [==============================] - 94s 1s/step - loss: 0.1427 - accuracy: 0.9454 - auc: 0.9870 - val_loss: 0.1376 - val_accuracy: 0.9444 - val_auc: 0.9882\n",
            "Epoch 51/100\n",
            "92/92 [==============================] - 94s 1s/step - loss: 0.1387 - accuracy: 0.9468 - auc: 0.9881 - val_loss: 0.1989 - val_accuracy: 0.9228 - val_auc: 0.9840\n",
            "Epoch 52/100\n",
            "92/92 [==============================] - 95s 1s/step - loss: 0.1220 - accuracy: 0.9541 - auc: 0.9899 - val_loss: 0.8354 - val_accuracy: 0.7052 - val_auc: 0.8856\n",
            "\n",
            "Epoch 00052: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 53/100\n",
            "92/92 [==============================] - 95s 1s/step - loss: 0.1061 - accuracy: 0.9622 - auc: 0.9929 - val_loss: 0.1351 - val_accuracy: 0.9460 - val_auc: 0.9883\n",
            "Epoch 54/100\n",
            "92/92 [==============================] - 95s 1s/step - loss: 0.0922 - accuracy: 0.9695 - auc: 0.9943 - val_loss: 0.2092 - val_accuracy: 0.9059 - val_auc: 0.9769\n",
            "Epoch 55/100\n",
            "92/92 [==============================] - 96s 1s/step - loss: 0.1233 - accuracy: 0.9488 - auc: 0.9910 - val_loss: 0.1102 - val_accuracy: 0.9614 - val_auc: 0.9923\n",
            "Epoch 56/100\n",
            "92/92 [==============================] - 96s 1s/step - loss: 0.1057 - accuracy: 0.9636 - auc: 0.9926 - val_loss: 0.1466 - val_accuracy: 0.9367 - val_auc: 0.9904\n",
            "Epoch 57/100\n",
            "92/92 [==============================] - 95s 1s/step - loss: 0.0955 - accuracy: 0.9642 - auc: 0.9935 - val_loss: 0.1047 - val_accuracy: 0.9583 - val_auc: 0.9935\n",
            "Epoch 58/100\n",
            "92/92 [==============================] - 97s 1s/step - loss: 0.1070 - accuracy: 0.9662 - auc: 0.9916 - val_loss: 0.1156 - val_accuracy: 0.9614 - val_auc: 0.9915\n",
            "Epoch 59/100\n",
            "92/92 [==============================] - 98s 1s/step - loss: 0.1114 - accuracy: 0.9619 - auc: 0.9911 - val_loss: 0.1440 - val_accuracy: 0.9398 - val_auc: 0.9894\n",
            "Epoch 60/100\n",
            "92/92 [==============================] - 97s 1s/step - loss: 0.1017 - accuracy: 0.9627 - auc: 0.9931 - val_loss: 0.1379 - val_accuracy: 0.9460 - val_auc: 0.9884\n",
            "\n",
            "Epoch 00060: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "Epoch 61/100\n",
            "92/92 [==============================] - 96s 1s/step - loss: 0.0867 - accuracy: 0.9714 - auc: 0.9951 - val_loss: 0.0924 - val_accuracy: 0.9676 - val_auc: 0.9939\n",
            "Epoch 62/100\n",
            "92/92 [==============================] - 95s 1s/step - loss: 0.0779 - accuracy: 0.9738 - auc: 0.9960 - val_loss: 0.0924 - val_accuracy: 0.9722 - val_auc: 0.9944\n",
            "Epoch 63/100\n",
            "92/92 [==============================] - 94s 1s/step - loss: 0.0846 - accuracy: 0.9731 - auc: 0.9941 - val_loss: 0.1764 - val_accuracy: 0.9336 - val_auc: 0.9811\n",
            "Epoch 64/100\n",
            "92/92 [==============================] - 95s 1s/step - loss: 0.0838 - accuracy: 0.9743 - auc: 0.9946 - val_loss: 0.0759 - val_accuracy: 0.9784 - val_auc: 0.9959\n",
            "Epoch 65/100\n",
            "92/92 [==============================] - 95s 1s/step - loss: 0.0984 - accuracy: 0.9646 - auc: 0.9935 - val_loss: 0.1141 - val_accuracy: 0.9599 - val_auc: 0.9937\n",
            "Epoch 66/100\n",
            "92/92 [==============================] - 95s 1s/step - loss: 0.0775 - accuracy: 0.9758 - auc: 0.9958 - val_loss: 0.0960 - val_accuracy: 0.9753 - val_auc: 0.9917\n",
            "Epoch 67/100\n",
            "92/92 [==============================] - 95s 1s/step - loss: 0.0974 - accuracy: 0.9651 - auc: 0.9933 - val_loss: 0.0931 - val_accuracy: 0.9599 - val_auc: 0.9952\n",
            "Epoch 68/100\n",
            "92/92 [==============================] - 96s 1s/step - loss: 0.0845 - accuracy: 0.9718 - auc: 0.9940 - val_loss: 0.2377 - val_accuracy: 0.9043 - val_auc: 0.9702\n",
            "Epoch 69/100\n",
            "92/92 [==============================] - 97s 1s/step - loss: 0.1020 - accuracy: 0.9661 - auc: 0.9923 - val_loss: 0.0927 - val_accuracy: 0.9676 - val_auc: 0.9937\n",
            "\n",
            "Epoch 00069: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "Epoch 70/100\n",
            "92/92 [==============================] - 96s 1s/step - loss: 0.0803 - accuracy: 0.9698 - auc: 0.9952 - val_loss: 0.0772 - val_accuracy: 0.9738 - val_auc: 0.9968\n",
            "Epoch 71/100\n",
            "92/92 [==============================] - 95s 1s/step - loss: 0.0836 - accuracy: 0.9734 - auc: 0.9952 - val_loss: 0.1003 - val_accuracy: 0.9645 - val_auc: 0.9944\n",
            "Epoch 72/100\n",
            "92/92 [==============================] - 95s 1s/step - loss: 0.0750 - accuracy: 0.9739 - auc: 0.9960 - val_loss: 0.0885 - val_accuracy: 0.9691 - val_auc: 0.9938\n",
            "Epoch 73/100\n",
            "92/92 [==============================] - 96s 1s/step - loss: 0.0763 - accuracy: 0.9736 - auc: 0.9962 - val_loss: 0.0633 - val_accuracy: 0.9784 - val_auc: 0.9983\n",
            "Epoch 74/100\n",
            "92/92 [==============================] - 96s 1s/step - loss: 0.0659 - accuracy: 0.9773 - auc: 0.9970 - val_loss: 0.0745 - val_accuracy: 0.9815 - val_auc: 0.9959\n",
            "Epoch 75/100\n",
            "92/92 [==============================] - 95s 1s/step - loss: 0.0778 - accuracy: 0.9715 - auc: 0.9953 - val_loss: 0.0781 - val_accuracy: 0.9660 - val_auc: 0.9947\n",
            "Epoch 76/100\n",
            "92/92 [==============================] - 95s 1s/step - loss: 0.0710 - accuracy: 0.9761 - auc: 0.9969 - val_loss: 0.0497 - val_accuracy: 0.9815 - val_auc: 0.9988\n",
            "Epoch 77/100\n",
            "92/92 [==============================] - 97s 1s/step - loss: 0.0782 - accuracy: 0.9750 - auc: 0.9957 - val_loss: 0.0666 - val_accuracy: 0.9769 - val_auc: 0.9976\n",
            "Epoch 78/100\n",
            "92/92 [==============================] - 97s 1s/step - loss: 0.0718 - accuracy: 0.9730 - auc: 0.9965 - val_loss: 0.1145 - val_accuracy: 0.9552 - val_auc: 0.9924\n",
            "Epoch 79/100\n",
            "92/92 [==============================] - 97s 1s/step - loss: 0.0797 - accuracy: 0.9739 - auc: 0.9944 - val_loss: 0.1012 - val_accuracy: 0.9614 - val_auc: 0.9930\n",
            "\n",
            "Epoch 00079: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "Epoch 80/100\n",
            "92/92 [==============================] - 97s 1s/step - loss: 0.0724 - accuracy: 0.9779 - auc: 0.9956 - val_loss: 0.0745 - val_accuracy: 0.9769 - val_auc: 0.9954\n",
            "Epoch 81/100\n",
            "92/92 [==============================] - 96s 1s/step - loss: 0.0721 - accuracy: 0.9746 - auc: 0.9964 - val_loss: 0.0488 - val_accuracy: 0.9846 - val_auc: 0.9987\n",
            "Epoch 82/100\n",
            "92/92 [==============================] - 96s 1s/step - loss: 0.0639 - accuracy: 0.9791 - auc: 0.9970 - val_loss: 0.0867 - val_accuracy: 0.9691 - val_auc: 0.9942\n",
            "Epoch 83/100\n",
            "92/92 [==============================] - 97s 1s/step - loss: 0.0688 - accuracy: 0.9759 - auc: 0.9954 - val_loss: 0.0694 - val_accuracy: 0.9722 - val_auc: 0.9970\n",
            "Epoch 84/100\n",
            "92/92 [==============================] - 97s 1s/step - loss: 0.0658 - accuracy: 0.9786 - auc: 0.9967 - val_loss: 0.0593 - val_accuracy: 0.9722 - val_auc: 0.9981\n",
            "Epoch 85/100\n",
            "92/92 [==============================] - 98s 1s/step - loss: 0.0614 - accuracy: 0.9800 - auc: 0.9976 - val_loss: 0.0737 - val_accuracy: 0.9799 - val_auc: 0.9960\n",
            "Epoch 86/100\n",
            "92/92 [==============================] - 96s 1s/step - loss: 0.0680 - accuracy: 0.9766 - auc: 0.9969 - val_loss: 0.0504 - val_accuracy: 0.9799 - val_auc: 0.9985\n",
            "\n",
            "Epoch 00086: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "Epoch 87/100\n",
            "92/92 [==============================] - 96s 1s/step - loss: 0.0677 - accuracy: 0.9770 - auc: 0.9959 - val_loss: 0.0761 - val_accuracy: 0.9769 - val_auc: 0.9955\n",
            "Epoch 88/100\n",
            "92/92 [==============================] - 96s 1s/step - loss: 0.0618 - accuracy: 0.9768 - auc: 0.9976 - val_loss: 0.0504 - val_accuracy: 0.9861 - val_auc: 0.9985\n",
            "Epoch 89/100\n",
            "92/92 [==============================] - 97s 1s/step - loss: 0.0796 - accuracy: 0.9732 - auc: 0.9957 - val_loss: 0.0734 - val_accuracy: 0.9645 - val_auc: 0.9974\n",
            "Epoch 90/100\n",
            "92/92 [==============================] - 97s 1s/step - loss: 0.0678 - accuracy: 0.9779 - auc: 0.9965 - val_loss: 0.0724 - val_accuracy: 0.9784 - val_auc: 0.9958\n",
            "Epoch 91/100\n",
            "92/92 [==============================] - 99s 1s/step - loss: 0.0658 - accuracy: 0.9768 - auc: 0.9966 - val_loss: 0.0662 - val_accuracy: 0.9784 - val_auc: 0.9969\n",
            "Epoch 92/100\n",
            "92/92 [==============================] - 98s 1s/step - loss: 0.0647 - accuracy: 0.9768 - auc: 0.9971 - val_loss: 0.0784 - val_accuracy: 0.9707 - val_auc: 0.9963\n",
            "Epoch 93/100\n",
            "92/92 [==============================] - 98s 1s/step - loss: 0.0569 - accuracy: 0.9808 - auc: 0.9979 - val_loss: 0.0583 - val_accuracy: 0.9753 - val_auc: 0.9981\n",
            "\n",
            "Epoch 00093: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "Epoch 94/100\n",
            "92/92 [==============================] - 98s 1s/step - loss: 0.0545 - accuracy: 0.9834 - auc: 0.9978 - val_loss: 0.0661 - val_accuracy: 0.9799 - val_auc: 0.9966\n",
            "Epoch 95/100\n",
            "92/92 [==============================] - 98s 1s/step - loss: 0.0623 - accuracy: 0.9806 - auc: 0.9972 - val_loss: 0.0700 - val_accuracy: 0.9753 - val_auc: 0.9960\n",
            "Epoch 96/100\n",
            "92/92 [==============================] - 98s 1s/step - loss: 0.0597 - accuracy: 0.9813 - auc: 0.9971 - val_loss: 0.0435 - val_accuracy: 0.9846 - val_auc: 0.9991\n",
            "Epoch 97/100\n",
            "92/92 [==============================] - 98s 1s/step - loss: 0.0659 - accuracy: 0.9805 - auc: 0.9964 - val_loss: 0.0934 - val_accuracy: 0.9568 - val_auc: 0.9946\n",
            "Epoch 98/100\n",
            "92/92 [==============================] - 96s 1s/step - loss: 0.0701 - accuracy: 0.9751 - auc: 0.9965 - val_loss: 0.0671 - val_accuracy: 0.9722 - val_auc: 0.9974\n",
            "\n",
            "Epoch 00098: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "Epoch 99/100\n",
            "92/92 [==============================] - 97s 1s/step - loss: 0.0683 - accuracy: 0.9810 - auc: 0.9963 - val_loss: 0.0634 - val_accuracy: 0.9769 - val_auc: 0.9976\n",
            "Epoch 100/100\n",
            "92/92 [==============================] - 98s 1s/step - loss: 0.0627 - accuracy: 0.9801 - auc: 0.9970 - val_loss: 0.0497 - val_accuracy: 0.9861 - val_auc: 0.9983\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0l1fuKUn4Ld"
      },
      "source": [
        "model.save(os.path.join(model_dir, 'casting_V3(100 eps).h5'))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxAVY77vFM4-"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.plot(history.history['auc'])\n",
        "plt.plot(history.history['val_auc'])\n",
        "plt.show()\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.show()\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1P0NR7LLT6y"
      },
      "source": [
        "\n",
        "### Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96ma34YeHq91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94a7f12d-4e86-4c98-bf27-fd49316da815"
      },
      "source": [
        "model = keras.models.load_model(os.path.join(model_dir, 'casting_V2.h5'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
            "Your GPU may run slowly with dtype policy mixed_float16 because it does not have compute capability of at least 7.0. Your GPU:\n",
            "  Tesla K80, compute capability 3.7\n",
            "See https://developer.nvidia.com/cuda-gpus for a list of GPUs and their compute capabilities.\n",
            "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d_30 (Conv2D)           (None, 222, 222, 8)       224       \n",
            "_________________________________________________________________\n",
            "conv2d_31 (Conv2D)           (None, 220, 220, 8)       584       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 110, 110, 8)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 110, 110, 8)       32        \n",
            "_________________________________________________________________\n",
            "conv2d_32 (Conv2D)           (None, 108, 108, 16)      1168      \n",
            "_________________________________________________________________\n",
            "conv2d_33 (Conv2D)           (None, 106, 106, 16)      2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 53, 53, 16)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 53, 53, 16)        64        \n",
            "_________________________________________________________________\n",
            "conv2d_34 (Conv2D)           (None, 52, 52, 16)        1040      \n",
            "_________________________________________________________________\n",
            "conv2d_35 (Conv2D)           (None, 50, 50, 16)        2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 25, 25, 16)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 25, 25, 16)        64        \n",
            "_________________________________________________________________\n",
            "conv2d_36 (Conv2D)           (None, 24, 24, 32)        2080      \n",
            "_________________________________________________________________\n",
            "conv2d_37 (Conv2D)           (None, 22, 22, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 11, 11, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 11, 11, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_38 (Conv2D)           (None, 11, 11, 16)        528       \n",
            "_________________________________________________________________\n",
            "conv2d_39 (Conv2D)           (None, 11, 11, 1)         17        \n",
            "_________________________________________________________________\n",
            "tf.math.sigmoid_3 (TFOpLambd (None, 11, 11, 1)         0         \n",
            "_________________________________________________________________\n",
            "global_max_pooling2d_3 (Glob (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 19,817\n",
            "Trainable params: 19,673\n",
            "Non-trainable params: 144\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gro2RJZouEs6",
        "outputId": "0ff9097d-f7d3-41c9-b4d8-c914606a8546"
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "test_generator = generator.flow_from_directory( os.path.join(dataset, 'test'), \n",
        "                                                target_size=(224, 224), \n",
        "                                                batch_size=batch_size,\n",
        "                                                class_mode='binary') "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 734 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6kFM1rTpkBB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa0261dc-7624-4c97-c820-ac1e99d4daed"
      },
      "source": [
        "attribute_to_idx = test_generator.class_indices\n",
        "idx_to_attribute = {value:key for key,value in attribute_to_idx.items()}\n",
        "\n",
        "print(attribute_to_idx)\n",
        "print(idx_to_attribute)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'def_front': 0, 'ok_front': 1}\n",
            "{0: 'def_front', 1: 'ok_front'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dV7_zvvzm3v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "outputId": "cdb4caa3-ef42-40d8-8be5-7e06cf58eb82"
      },
      "source": [
        "scores = model.evaluate_generator(test_generator, verbose=1)\n",
        "scores_keys = ['loss', 'accuracy', 'auc']\n",
        "\n",
        "for key,score in zip(scores_keys, scores):\n",
        "\n",
        "    print(key, ':', score)\n",
        "\n",
        "\"\"\"\n",
        "50 eps\n",
        "\n",
        "loss : 0.11778640002012253\n",
        "accuracy : 0.9632152318954468\n",
        "auc : 0.9908440709114075\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "100 eps\n",
        "\n",
        "loss : 0.04719378054141998\n",
        "accuracy : 0.9863760471343994\n",
        "auc : 0.9984898567199707\n",
        "\"\"\""
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1877: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
            "  warnings.warn('`Model.evaluate_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "12/12 [==============================] - 694s 63s/step - loss: 0.0472 - accuracy: 0.9864 - auc: 0.9985\n",
            "loss : 0.04719378054141998\n",
            "accuracy : 0.9863760471343994\n",
            "auc : 0.9984898567199707\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n50 eps\\n\\nloss : 0.11778640002012253\\naccuracy : 0.9632152318954468\\nauc : 0.9908440709114075\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DELw_VzGLWW3"
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "for idx, (images, output) in enumerate(test_generator):\n",
        "\n",
        "    if idx == 2:\n",
        "        break\n",
        "\n",
        "    for i in range(batch_size):\n",
        "\n",
        "        image = images[i]\n",
        "\n",
        "        cv2_imshow(image * 255)\n",
        "\n",
        "        preds = model.predict(np.expand_dims(image, axis=0))[0][0]\n",
        "\n",
        "        actual_value = output[i]\n",
        "\n",
        "        predicted_value = (preds > 0.75).astype(np.int)\n",
        "\n",
        "        if predicted_value == 0:\n",
        "\n",
        "            confidence = (1- preds) * 100\n",
        "        else:\n",
        "            confidence = preds * 100\n",
        "\n",
        "        print(\"Confidence\", confidence, \"%\")\n",
        "        print(\"Actual_value\", idx_to_attribute[int(actual_value)])\n",
        "        print(\"Predicted value\", idx_to_attribute[int(predicted_value)])\n",
        "    \n",
        "    idx += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVSR032UdCvf"
      },
      "source": [
        "#### Get Activation maps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfVxtpRzdHtJ"
      },
      "source": [
        "def get_img_array(img_path, size):\n",
        "    # `img` is a PIL image of size 299x299\n",
        "    img = keras.preprocessing.image.load_img(img_path, target_size=size)\n",
        "    # `array` is a float32 Numpy array of shape (299, 299, 3)\n",
        "    array = keras.preprocessing.image.img_to_array(img)\n",
        "    # We add a dimension to transform our array into a \"batch\"\n",
        "    # of size (1, 299, 299, 3)\n",
        "    array = np.expand_dims(array, axis=0)\n",
        "    return array\n",
        "\n",
        "\n",
        "def make_gradcam_heatmap(\n",
        "    img_array, model, last_conv_layer, classifier_layer\n",
        "):\n",
        "    # First, we create a model that maps the input image to the activations\n",
        "    # of the last conv layer\n",
        "    \n",
        "    last_conv_layer_model = keras.Model(model.inputs, last_conv_layer.output)\n",
        "\n",
        "    # Second, we create a model that maps the activations of the last conv\n",
        "    # layer to the final class predictions\n",
        "    classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])\n",
        "    x = classifier_input\n",
        "    x = classifier_layer(x)\n",
        "    classifier_model = keras.Model(classifier_input, x)\n",
        "\n",
        "    # Then, we compute the gradient of the top predicted class for our input image\n",
        "    # with respect to the activations of the last conv layer\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Compute activations of the last conv layer and make the tape watch it\n",
        "        last_conv_layer_output = last_conv_layer_model(img_array)\n",
        "        tape.watch(last_conv_layer_output)\n",
        "        # Compute class predictions\n",
        "        preds = classifier_model(last_conv_layer_output)\n",
        "        top_pred_index = tf.argmax(preds[0])\n",
        "        top_class_channel = preds[:, top_pred_index]\n",
        "\n",
        "    # This is the gradient of the top predicted class with regard to\n",
        "    # the output feature map of the last conv layer\n",
        "    grads = tape.gradient(top_class_channel, last_conv_layer_output)\n",
        "\n",
        "    # This is a vector where each entry is the mean intensity of the gradient\n",
        "    # over a specific feature map channel\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    # We multiply each channel in the feature map array\n",
        "    # by \"how important this channel is\" with regard to the top predicted class\n",
        "    last_conv_layer_output = last_conv_layer_output.numpy()[0]\n",
        "    pooled_grads = pooled_grads.numpy()\n",
        "    for i in range(pooled_grads.shape[-1]):\n",
        "        last_conv_layer_output[:, :, i] *= pooled_grads[i]\n",
        "\n",
        "    # The channel-wise mean of the resulting feature map\n",
        "    # is our heatmap of class activation\n",
        "    heatmap = np.mean(last_conv_layer_output, axis=-1)\n",
        "\n",
        "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
        "    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n",
        "    return heatmap"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PlFTBWR0DHJ"
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "last_conv_layer = model.layers[-3]\n",
        "classifier_layer = model.layers[-1]\n",
        "\n",
        "for idx, (images, output) in enumerate(test_generator):\n",
        "\n",
        "    if idx == 1:\n",
        "        break\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        img = images[i] * 255\n",
        "\n",
        "        image = np.expand_dims(images[i], axis=0)\n",
        "\n",
        "        preds = model.predict(image, verbose=1)[0][0]\n",
        "\n",
        "        actual_value = output[i]\n",
        "\n",
        "        predicted_value = (preds > 0.6).astype(np.int)\n",
        "\n",
        "        if predicted_value == 0:\n",
        "\n",
        "            confidence = (1- preds) * 100\n",
        "        else:\n",
        "            confidence = preds * 100\n",
        "\n",
        "       \n",
        "        # Generate class activation heatmap\n",
        "        heatmap = make_gradcam_heatmap(\n",
        "            image, model, last_conv_layer, classifier_layer\n",
        "        )\n",
        "\n",
        "\n",
        "        # We rescale heatmap to a range 0-255\n",
        "        heatmap = np.uint8(255 * heatmap)\n",
        "\n",
        "        # We use jet colormap to colorize heatmap\n",
        "        jet = cm.get_cmap(\"jet\")\n",
        "\n",
        "        # We use RGB values of the colormap\n",
        "        jet_colors = jet(np.arange(256))[:, :3]\n",
        "        jet_heatmap = jet_colors[heatmap]\n",
        "\n",
        "        # We create an image with RGB colorized heatmap\n",
        "        jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n",
        "        jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
        "        jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n",
        "\n",
        "        # Superimpose the heatmap on original image\n",
        "        superimposed_img = jet_heatmap * 0.6 + img\n",
        "        superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n",
        "\n",
        "        plt.figure(figsize=(5,5))\n",
        "        plt.imshow(superimposed_img)\n",
        "        plt.show()\n",
        "\n",
        "        print(\"Confidence\", confidence, \"%\")\n",
        "        print(\"Actual_value\", idx_to_attribute[int(actual_value)])\n",
        "        print(\"Predicted value\", idx_to_attribute[int(predicted_value)])\n",
        "    \n",
        "        \n",
        "    idx += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbxmdITkL2TX"
      },
      "source": [
        "## SOLAR PANEL DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSoM7XTQL5yS"
      },
      "source": [
        "### Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIHmBY-nL5Kb"
      },
      "source": [
        "import os\n",
        "import tarfile\n",
        "import shutil\n",
        "import numpy as np\n",
        "\n",
        "dataset_dir = '/content/drive/MyDrive/Major Project/Datasets/datasets'\n",
        "\n",
        "model_dir = '/content/drive/MyDrive/Major Project/Models/V1'\n",
        "\n",
        "if not os.path.exists(model_dir):\n",
        "    os.mkdir(model_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtcvlhoacpY7"
      },
      "source": [
        "filename = 'solar_panels_products.tar.xz'\n",
        "tar_file = os.path.join(dataset_dir, filename)\n",
        "\n",
        "my_tar = tarfile.open(tar_file)\n",
        "my_tar.extractall(dataset_dir) # specify which folder to extract to\n",
        "my_tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihWOG3lSdHTA"
      },
      "source": [
        "dataset = os.path.join(dataset_dir, 'solar_panels_products')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPjUfrM9dOGv"
      },
      "source": [
        "generator = keras.preprocessing.image.ImageDataGenerator(\n",
        "                            \n",
        "                            rotation_range=10,\n",
        "                            width_shift_range=0.1,\n",
        "                            height_shift_range=0.1,\n",
        "                            brightness_range = [0.5,1.0],\n",
        "                            zoom_range=0.1,\n",
        "                            rescale=1./255,\n",
        "                            fill_mode=\"nearest\",\n",
        "                            cval=0.0,\n",
        "                            horizontal_flip=True,\n",
        "                            vertical_flip=True,\n",
        "                            validation_split=0.1,\n",
        "                            dtype='float64',\n",
        "                        )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnWkTdw8dR1v"
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_generator = generator.flow_from_directory( os.path.join(dataset, 'train'), \n",
        "                                                target_size=(224, 224), \n",
        "                                                batch_size=batch_size,\n",
        "                                                class_mode='binary',\n",
        "                                                subset='training',\n",
        "                                                shuffle=True) \n",
        "                                            \n",
        "val_generator = generator.flow_from_directory( os.path.join(dataset, 'train'), \n",
        "                                                target_size=(224, 224), \n",
        "                                                batch_size=batch_size,\n",
        "                                                class_mode='binary',\n",
        "                                                subset='validation',\n",
        "                                                shuffle=True) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCKruVOadOJq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xskYOOLoL7ol"
      },
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbQGcviwL8pP"
      },
      "source": [
        "epochs = 100\n",
        "\n",
        "history = model.fit(train_generator, \n",
        "                    epochs= epochs,\n",
        "                    validation_data = val_generator,\n",
        "                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4j7ujMhXdZLC"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.plot(history.history['auc'])\n",
        "plt.plot(history.history['val_auc'])\n",
        "plt.show()\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ux2OtQtdZN1"
      },
      "source": [
        "model.save(os.path.join(model_dir, 'casting.h5'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjgkntmwdupb"
      },
      "source": [
        "### Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdCpMoV4wwLP"
      },
      "source": [
        "test_generator = generator.flow_from_directory( os.path.join(dataset, 'test'), \n",
        "                                                target_size=(224, 224), \n",
        "                                                batch_size=batch_size,\n",
        "                                                class_mode='binary',\n",
        "                                                shuffle=True) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTYizERedt3-"
      },
      "source": [
        "attribute_to_idx = test_generator.class_indices\n",
        "idx_to_attribute = {value:key for key,value in attribute_to_idx.items()}\n",
        "\n",
        "print(attribute_to_idx)\n",
        "print(idx_to_attribute)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ct6X2E74dz26"
      },
      "source": [
        "scores = model.evaluate_generator(test_generator, verbose=1)\n",
        "print(scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Sc-OlBKdz54"
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "for idx, (images, output) in enumerate(test_generator):\n",
        "\n",
        "    if idx == 1:\n",
        "        break\n",
        "\n",
        "    for i in range(batch_size):\n",
        "\n",
        "        image = images[i]\n",
        "\n",
        "        cv2_imshow(image * 255)\n",
        "\n",
        "        confidence = model.predict(np.expand_dims(image, axis=0))[0]\n",
        "\n",
        "        actual_value = output[i]\n",
        "        \n",
        "        predicted_value = (confidence > 0.5).astype(np.int)\n",
        "\n",
        "        print(\"Confidence\", confidence)\n",
        "        print(idx_to_attribute[int(actual_value)])\n",
        "        print(idx_to_attribute[int(predicted_value)])\n",
        "    \n",
        "    idx += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGZyKHQDL9V5"
      },
      "source": [
        "## STEEL DEFECT DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpueROgeMAmA"
      },
      "source": [
        "### Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9gef8BJL_73"
      },
      "source": [
        "import os\n",
        "import tarfile\n",
        "import shutil\n",
        "import numpy as np\n",
        "\n",
        "dataset_dir = '/content/drive/MyDrive/Major Project/Datasets/datasets'\n",
        "\n",
        "model_dir = '/content/drive/MyDrive/Major Project/Models/V1'\n",
        "\n",
        "if not os.path.exists(model_dir):\n",
        "    os.mkdir(model_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAdNOTKXfkLI"
      },
      "source": [
        "filename = 'solar_panels_products.tar.xz'\n",
        "tar_file = os.path.join(dataset_dir, filename)\n",
        "\n",
        "my_tar = tarfile.open(tar_file)\n",
        "my_tar.extractall(dataset_dir) # specify which folder to extract to\n",
        "my_tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zTKW4PTfkOH"
      },
      "source": [
        "dataset = os.path.join(dataset_dir, 'casting_dataset')\n",
        "\n",
        "generator = keras.preprocessing.image.ImageDataGenerator(\n",
        "                            \n",
        "                            rotation_range=10,\n",
        "                            width_shift_range=0.1,\n",
        "                            height_shift_range=0.1,\n",
        "                            brightness_range = [0.5,1.0],\n",
        "                            zoom_range=0.1,\n",
        "                            rescale=1./255,\n",
        "                            fill_mode=\"nearest\",\n",
        "                            cval=0.0,\n",
        "                            horizontal_flip=True,\n",
        "                            vertical_flip=True,\n",
        "                            validation_split=0.1,\n",
        "                            dtype='float64',\n",
        "                        )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZZvuIsLfkQ7"
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_generator = generator.flow_from_directory( os.path.join(dataset, 'train'), \n",
        "                                                target_size=(224, 224), \n",
        "                                                batch_size=batch_size,\n",
        "                                                class_mode='binary',\n",
        "                                                subset='training') \n",
        "                                            \n",
        "val_generator = generator.flow_from_directory( os.path.join(dataset, 'train'), \n",
        "                                                target_size=(224, 224), \n",
        "                                                batch_size=batch_size,\n",
        "                                                class_mode='binary',\n",
        "                                                subset='validation') \n",
        "\n",
        "test_generator = generator.flow_from_directory( os.path.join(dataset, 'test'), \n",
        "                                                target_size=(224, 224), \n",
        "                                                batch_size=batch_size,\n",
        "                                                class_mode='binary') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tTXLmvvfkTS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCcoJNXKMDAZ"
      },
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AP8Y1J7QMD8W"
      },
      "source": [
        "epochs = 100\n",
        "\n",
        "history = model.fit(train_generator, \n",
        "                    epochs= epochs,\n",
        "                    validation_data = val_generator,\n",
        "                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jkckAwvhE2h"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.plot(history.history['auc'])\n",
        "plt.plot(history.history['val_auc'])\n",
        "plt.show()\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roP26cLXhIpX"
      },
      "source": [
        "model.save(os.path.join(model_dir, 'casting.h5'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iK0goCVbf2d2"
      },
      "source": [
        "### Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWUqOnJzf4je"
      },
      "source": [
        "attribute_to_idx = test_generator.class_indices\n",
        "idx_to_attribute = {value:key for key,value in attribute_to_idx.items()}\n",
        "\n",
        "print(attribute_to_idx)\n",
        "print(idx_to_attribute)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMCxzvJVf4m1"
      },
      "source": [
        "scores = model.evaluate_generator(test_generator, verbose=1)\n",
        "print(scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nkpoi34xf4q6"
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "for idx, (images, output) in enumerate(test_generator):\n",
        "\n",
        "    if idx == 1:\n",
        "        break\n",
        "\n",
        "    for i in range(batch_size):\n",
        "\n",
        "        image = images[i]\n",
        "\n",
        "        cv2_imshow(image * 255)\n",
        "\n",
        "        confidence = model.predict(np.expand_dims(image, axis=0))[0]\n",
        "\n",
        "        actual_value = output[i]\n",
        "        \n",
        "        predicted_value = (confidence > 0.5).astype(np.int)\n",
        "\n",
        "        print(\"Confidence\", confidence)\n",
        "        print(idx_to_attribute[int(actual_value)])\n",
        "        print(idx_to_attribute[int(predicted_value)])\n",
        "    \n",
        "    idx += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZzyozMIjPTQ"
      },
      "source": [
        "## SURFACE DEFECT DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOJrXts9jPTZ"
      },
      "source": [
        "### Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8-D77KfjPTb"
      },
      "source": [
        "import os\n",
        "import tarfile\n",
        "import shutil\n",
        "import numpy as np\n",
        "\n",
        "dataset_dir = '/content/drive/MyDrive/Major Project/Datasets/datasets'\n",
        "\n",
        "model_dir = '/content/drive/MyDrive/Major Project/Models/V1'\n",
        "\n",
        "if not os.path.exists(model_dir):\n",
        "    os.mkdir(model_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HACAXNOdjPTc"
      },
      "source": [
        "filename = 'solar_panels_products.tar.xz'\n",
        "tar_file = os.path.join(dataset_dir, filename)\n",
        "\n",
        "my_tar = tarfile.open(tar_file)\n",
        "my_tar.extractall(dataset_dir) # specify which folder to extract to\n",
        "my_tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTBGqZrljPTd"
      },
      "source": [
        "dataset = os.path.join(dataset_dir, 'casting_dataset')\n",
        "\n",
        "generator = keras.preprocessing.image.ImageDataGenerator(\n",
        "                            \n",
        "                            rotation_range=10,\n",
        "                            width_shift_range=0.1,\n",
        "                            height_shift_range=0.1,\n",
        "                            brightness_range = [0.5,1.0],\n",
        "                            zoom_range=0.1,\n",
        "                            rescale=1./255,\n",
        "                            fill_mode=\"nearest\",\n",
        "                            cval=0.0,\n",
        "                            horizontal_flip=True,\n",
        "                            vertical_flip=True,\n",
        "                            validation_split=0.1,\n",
        "                            dtype='float64',\n",
        "                        )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKfOCpqYjPTd"
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_generator = generator.flow_from_directory( os.path.join(dataset, 'train'), \n",
        "                                                target_size=(224, 224), \n",
        "                                                batch_size=batch_size,\n",
        "                                                class_mode='binary',\n",
        "                                                subset='training') \n",
        "                                            \n",
        "val_generator = generator.flow_from_directory( os.path.join(dataset, 'train'), \n",
        "                                                target_size=(224, 224), \n",
        "                                                batch_size=batch_size,\n",
        "                                                class_mode='binary',\n",
        "                                                subset='validation') \n",
        "\n",
        "test_generator = generator.flow_from_directory( os.path.join(dataset, 'test'), \n",
        "                                                target_size=(224, 224), \n",
        "                                                batch_size=batch_size,\n",
        "                                                class_mode='binary') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frdg4R1BjPTf"
      },
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbNChot7jPTf"
      },
      "source": [
        "epochs = 100\n",
        "\n",
        "history = model.fit(train_generator, \n",
        "                    epochs= epochs,\n",
        "                    validation_data = val_generator,\n",
        "                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glT4iZdAjPTf"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.plot(history.history['auc'])\n",
        "plt.plot(history.history['val_auc'])\n",
        "plt.show()\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tblv6fo0jPTg"
      },
      "source": [
        "model.save(os.path.join(model_dir, 'casting.h5'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brNW_0K5jPTg"
      },
      "source": [
        "### Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPJbLcLzjPTg"
      },
      "source": [
        "attribute_to_idx = test_generator.class_indices\n",
        "idx_to_attribute = {value:key for key,value in attribute_to_idx.items()}\n",
        "\n",
        "print(attribute_to_idx)\n",
        "print(idx_to_attribute)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4uqP_7tjPTh"
      },
      "source": [
        "scores = model.evaluate_generator(test_generator, verbose=1)\n",
        "print(scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxC0pqLBjPTh"
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "for idx, (images, output) in enumerate(test_generator):\n",
        "\n",
        "    if idx == 1:\n",
        "        break\n",
        "\n",
        "    for i in range(batch_size):\n",
        "\n",
        "        image = images[i]\n",
        "\n",
        "        cv2_imshow(image * 255)\n",
        "\n",
        "        confidence = model.predict(np.expand_dims(image, axis=0))[0]\n",
        "\n",
        "        actual_value = output[i]\n",
        "        \n",
        "        predicted_value = (confidence > 0.5).astype(np.int)\n",
        "\n",
        "        print(\"Confidence\", confidence)\n",
        "        print(idx_to_attribute[int(actual_value)])\n",
        "        print(idx_to_attribute[int(predicted_value)])\n",
        "    \n",
        "    idx += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSWVQhQHjQxW"
      },
      "source": [
        "## WELDING DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Tdx4b5ljQxX"
      },
      "source": [
        "### Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg06VjpxjQxX"
      },
      "source": [
        "import os\n",
        "import tarfile\n",
        "import shutil\n",
        "import numpy as np\n",
        "\n",
        "dataset_dir = '/content/drive/MyDrive/Major Project/Datasets/datasets'\n",
        "\n",
        "model_dir = '/content/drive/MyDrive/Major Project/Models/V1'\n",
        "\n",
        "if not os.path.exists(model_dir):\n",
        "    os.mkdir(model_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQWpTlN-jQxY"
      },
      "source": [
        "filename = 'solar_panels_products.tar.xz'\n",
        "tar_file = os.path.join(dataset_dir, filename)\n",
        "\n",
        "my_tar = tarfile.open(tar_file)\n",
        "my_tar.extractall(dataset_dir) # specify which folder to extract to\n",
        "my_tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GSJgacOjQxY"
      },
      "source": [
        "dataset = os.path.join(dataset_dir, 'casting_dataset')\n",
        "\n",
        "generator = keras.preprocessing.image.ImageDataGenerator(\n",
        "                            \n",
        "                            rotation_range=10,\n",
        "                            width_shift_range=0.1,\n",
        "                            height_shift_range=0.1,\n",
        "                            brightness_range = [0.5,1.0],\n",
        "                            zoom_range=0.1,\n",
        "                            rescale=1./255,\n",
        "                            fill_mode=\"nearest\",\n",
        "                            cval=0.0,\n",
        "                            horizontal_flip=True,\n",
        "                            vertical_flip=True,\n",
        "                            validation_split=0.1,\n",
        "                            dtype='float64',\n",
        "                        )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n29n0HZzjQxZ"
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_generator = generator.flow_from_directory( os.path.join(dataset, 'train'), \n",
        "                                                target_size=(224, 224), \n",
        "                                                batch_size=batch_size,\n",
        "                                                class_mode='binary',\n",
        "                                                subset='training') \n",
        "                                            \n",
        "val_generator = generator.flow_from_directory( os.path.join(dataset, 'train'), \n",
        "                                                target_size=(224, 224), \n",
        "                                                batch_size=batch_size,\n",
        "                                                class_mode='binary',\n",
        "                                                subset='validation') \n",
        "\n",
        "test_generator = generator.flow_from_directory( os.path.join(dataset, 'test'), \n",
        "                                                target_size=(224, 224), \n",
        "                                                batch_size=batch_size,\n",
        "                                                class_mode='binary') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXJotie0jQxa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TC_aUG4jQxa"
      },
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2sxMg2JjQxa"
      },
      "source": [
        "epochs = 100\n",
        "\n",
        "history = model.fit(train_generator, \n",
        "                    epochs= epochs,\n",
        "                    validation_data = val_generator,\n",
        "                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdkyunF1jQxb"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.plot(history.history['auc'])\n",
        "plt.plot(history.history['val_auc'])\n",
        "plt.show()\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HyUE136jQxb"
      },
      "source": [
        "model.save(os.path.join(model_dir, 'casting.h5'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv_VZykljQxc"
      },
      "source": [
        "### Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3KQLRpmjQxc"
      },
      "source": [
        "attribute_to_idx = test_generator.class_indices\n",
        "idx_to_attribute = {value:key for key,value in attribute_to_idx.items()}\n",
        "\n",
        "print(attribute_to_idx)\n",
        "print(idx_to_attribute)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AScEbsUJjQxc"
      },
      "source": [
        "scores = model.evaluate_generator(test_generator, verbose=1)\n",
        "print(scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CMQAlUfjQxc"
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "for idx, (images, output) in enumerate(test_generator):\n",
        "\n",
        "    if idx == 1:\n",
        "        break\n",
        "\n",
        "    for i in range(batch_size):\n",
        "\n",
        "        image = images[i]\n",
        "\n",
        "        cv2_imshow(image * 255)\n",
        "\n",
        "        confidence = model.predict(np.expand_dims(image, axis=0))[0]\n",
        "\n",
        "        actual_value = output[i]\n",
        "        \n",
        "        predicted_value = (confidence > 0.5).astype(np.int)\n",
        "\n",
        "        print(\"Confidence\", confidence)\n",
        "        print(idx_to_attribute[int(actual_value)])\n",
        "        print(idx_to_attribute[int(predicted_value)])\n",
        "    \n",
        "    idx += 1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}